From: George Kashperko <george@znau.edu.ua>

Broadcom SSB GMAC ethernet driver for Linux. No support for
gmacs bound to anything other than robo switch.
Signed-off-by: George Kashperko <george@znau.edu.ua>
---
 drivers/net/Kconfig          |    8 
 drivers/net/Makefile         |    1 
 drivers/net/bgmac.c          | 1390 ++++++++++++++++++++++++++++
 drivers/net/bgmac.h          |  506 ++++++++++
 drivers/ssb/Kconfig          |   28 
 drivers/ssb/Makefile         |    1 
 drivers/ssb/ssb_dma.c        | 1589 +++++++++++++++++++++++++++++++++
 include/linux/ssb/ssb_dma.h  |  489 ++++++++++
 include/linux/ssb/ssb_regs.h |    1 
 9 files changed, 4013 insertions(+)
--- linux-3.0.4.orig/drivers/net/bgmac.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-3.0.4/drivers/net/bgmac.c	2011-09-25 00:07:36.000000000 +0300
@@ -0,0 +1,1390 @@
+/*
+ * Broadcom SSB GMAC ethernet driver for Linux
+ *
+ * Copyright (C) 2011, George Kashperko <george@znau.edu.ua>
+ * Copyright (C) 2009, Broadcom Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+/*
+ * v1.0, 2011-02-04 20:55T+2
+ *  initial release with ext. Robo Switch support
+ * v1.1, 2011-05-07 02:30T+2
+ *  dma engine failures detection
+ * v1.2, 2011-05-17 01:37T+2
+ *  allow mii ioctls with netif down to address switch-robo detection failures
+ * v1.3, 2011-09-25 00:21T+2
+ *  address crashes on high loads
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/mii.h>
+#include <linux/if_vlan.h>
+#include <linux/slab.h>
+#include <linux/ssb/ssb.h>
+#include <linux/ssb/ssb_dma.h>
+#include <bcm47xx.h>
+
+// #define BGMAC_DEBUG
+#include "bgmac.h"
+
+#define BGMAC_MODULE_NAME "bgmac"
+#define BGMAC_MODULE_VERSION "1.3"
+#define BGMAC_MODULE_DESCRIPTION "Broadcom SSB GMAC ethernet driver"
+
+static const struct ssb_device_id bgmac_tbl[] = {
+	SSB_DEVICE(SSB_VENDOR_BROADCOM2, SSB_DEV_ETHERNET_GMAC, 0),
+	SSB_DEVTABLE_END
+};
+
+MODULE_AUTHOR("George Kashperko");
+MODULE_DESCRIPTION(BGMAC_MODULE_DESCRIPTION);
+MODULE_LICENSE("GPL");
+MODULE_VERSION(BGMAC_MODULE_VERSION);
+MODULE_DEVICE_TABLE(ssb, bgmac_tbl);
+
+/* dma channels' descriptions */
+static const struct ssb_dma_info bgmac_dma[] = {
+	SSB_DMA("RXDMA0", BGMAC_DMA(RX, 0), BGMAC_DMA_RX_SLOTS, DMA_FROM_DEVICE),
+	SSB_DMA("TXDMA0", BGMAC_DMA(TX, 0), BGMAC_DMA_TX_SLOTS, DMA_TO_DEVICE),
+	SSB_DMA("TXDMA1", BGMAC_DMA(TX, 1), BGMAC_DMA_TX_SLOTS, DMA_TO_DEVICE),
+	SSB_DMA("TXDMA2", BGMAC_DMA(TX, 2), BGMAC_DMA_TX_SLOTS, DMA_TO_DEVICE),
+	SSB_DMA("TXDMA3", BGMAC_DMA(TX, 3), BGMAC_DMA_TX_SLOTS, DMA_TO_DEVICE),
+	SSB_DMA_TABLE_END
+};
+
+/* forward declarations */
+static void
+bgmac_core_reset(struct bgmac_device *bgmac);
+static void
+bgmac_core_setup(struct bgmac_device *bgmac);
+static void
+bgmac_core_disable(struct bgmac_device *bgmac);
+static int
+bgmac_core_enable(struct bgmac_device *bgmac);
+static int
+bgmac_core_verify(struct bgmac_device *bgmac);
+
+/** general-purpose helpers **/
+static const char *
+bgmac_linkspeed_name(enum bgmac_linkspeed speed)
+{
+	static const char *linkspeed_name[BGMAC_LINKSPEED_MAX + 2] = {
+		"Auto",
+		"10 Mbps Full-Duplex",
+		"10 Mbps Half-Duplex",
+		"100 Mbps Full-Duplex",
+		"100 Mbps Half-Duplex",
+		"1000 Mbps Full-Duplex",
+		"1000 Mbps Half-Duplex",
+		"UNKNOWN",
+	};
+
+	if (speed >= BGMAC_LINKSPEED_AUTO &&
+	    speed < BGMAC_LINKSPEED_MAX)
+		return linkspeed_name[speed + 1];
+	else
+		return linkspeed_name[BGMAC_LINKSPEED_MAX + 1];
+}
+
+static const char *
+bgmac_phy_name(enum bgmac_phy_type type)
+{
+	static const char *phy_type_name[BGMAC_PHYTYPE_MAX + 1] = {
+		"Unknown or unupported phy",
+		"ext. Robo Switch",
+	};
+
+	if (type >= BGMAC_PHYTYPE_UNKNOWN &&
+	    type < BGMAC_PHYTYPE_MAX)
+		return phy_type_name[type + 1];
+	    else
+		return phy_type_name[0];
+}
+
+/** dma interface ops **/
+/* allocate skb buffer for dma rx channel */
+static void *
+bgmac_dma_buf_alloc(struct ssb_dma *dma, u16 length, void **data)
+{
+	struct sk_buff *skb;
+
+	BGMAC_BUG_ON(!length);
+	BGMAC_BUG_ON(!data);
+
+	skb = dev_alloc_skb(length);
+	*data = (void *)skb->data;
+	return skb;
+}
+
+/* free skb buffer */
+static void
+bgmac_dma_buf_free(struct ssb_dma *dma, void *buffer)
+{
+    	dev_kfree_skb(buffer);
+}
+
+/* dma interface ops struct */
+struct ssb_dma_driver_ops bgmac_dma_ops = {
+	.buf_alloc	= bgmac_dma_buf_alloc,
+	.buf_free	= bgmac_dma_buf_free,
+};
+
+/** mac mapped I/O registers access helpers **/
+
+/* 32-bit read mac register */
+static inline u32
+bgmac_read32(struct bgmac_device *dev, u16 offset)
+{
+	struct ssb_device *sdev = dev->ssb_dev;
+	return sdev->ops->read32(sdev, offset);
+}
+
+/* 32-bit write mac register */
+static inline void
+bgmac_write32(struct bgmac_device *dev, u16 offset, u32 value)
+{
+	struct ssb_device *sdev = dev->ssb_dev;
+	sdev->ops->write32(sdev, offset, value);
+}
+
+/* 32-bit mask-and-set mac individual register bits state */
+static inline void
+bgmac_maskset32(struct bgmac_device *dev, u16 offset, u32 mask, u32 value)
+{
+	struct ssb_device *sdev = dev->ssb_dev;
+	sdev->ops->write32(sdev, offset,
+			   (sdev->ops->read32(sdev, offset) & mask) | value);
+}
+
+/* 32-bit set mac individual register bits state */
+static inline void
+bgmac_set32(struct bgmac_device *dev, u16 offset, u32 value)
+{
+	struct ssb_device *sdev = dev->ssb_dev;
+	sdev->ops->write32(sdev, offset,
+			   sdev->ops->read32(sdev, offset) | value);
+}
+
+/* mac cmdcfg register helper.
+ * put mac to reset, modify bits configuration, bring mac
+ * back out of reset */
+static u32
+bgmac_cmdcfg(struct bgmac_device *bgmac, u32 mask, u32 cfg)
+{
+	u32 newcfg, oldcfg;
+
+	BGMAC_BUG_ON((mask | cfg) & BGMAC_CMDCFG_SR);
+
+	newcfg = oldcfg = bgmac_read32(bgmac, BGMAC_CMDCFG);
+
+	newcfg &= ~mask;
+	newcfg |= cfg;
+	if (newcfg == oldcfg)
+		return oldcfg;
+
+	bgmac_write32(bgmac, BGMAC_CMDCFG, oldcfg | BGMAC_CMDCFG_SR);
+	udelay(BGMAC_MAC_SWRESET_DELAY);
+
+	bgmac_write32(bgmac, BGMAC_CMDCFG, newcfg | BGMAC_CMDCFG_SR);
+
+	bgmac_write32(bgmac, BGMAC_CMDCFG, newcfg);
+	udelay(BGMAC_MAC_SWRESET_DELAY);
+
+	return newcfg;
+}
+
+/* wait for specific 32-bit mac register bit state
+ * @timeout: maximum wait time in tens of useconds
+ * @expected_state: true for truth expected value comparison, false otherwise.
+ * returns 0xFFFF on failure or register value on success */
+static u16
+bgmac_waitbit32(struct bgmac_device *bgmac, u16 offset, u32 bit,
+		u32 timeout, bool expected_state)
+{
+	u32 i, tmp;
+	for (i = 0; i < timeout; i++) {
+		tmp = bgmac_read32(bgmac, offset);
+		if (expected_state && (tmp & bit))
+			return tmp;
+		if (!expected_state && !(tmp & bit))
+			return tmp;
+		udelay(10);
+	}
+	BGMAC_WARN_ON(1);
+
+	return 0xFFFF;
+}
+
+/** phy register access helpers **/
+
+/* 16-bit read phy register
+ * returns 0xFFFF on failure */
+u16
+bgmac_phy_read16(struct bgmac_device *bgmac, u8 phy, u8 reg)
+{
+	BGMAC_BUG_ON(phy >= BGMAC_PHYACCESS_ADDR_MAX);
+	BGMAC_BUG_ON(reg >= BGMAC_PHYACCESS_REG_MAX);
+	/* issue the read */
+	bgmac_maskset32(bgmac, BGMAC_PHYCTL, ~BGMAC_PHYCTL_EPA_MASK, phy);
+	bgmac_write32(bgmac, BGMAC_PHYACCESS, BGMAC_PHYACCESS_START |
+					      BGMAC_PHYACCESS_ADDR(phy) |
+					      BGMAC_PHYACCESS_REG(reg));
+	/* wait for it to complete */
+	return bgmac_waitbit32(bgmac, BGMAC_PHYACCESS, BGMAC_PHYACCESS_START,
+			       100, false) & BGMAC_PHYACCESS_DATA_MASK;
+}
+
+/* 16-bit write phy register */
+void
+bgmac_phy_write16(struct bgmac_device *bgmac, u8 phy, u8 reg, u16 value)
+{
+	BGMAC_BUG_ON(phy >= BGMAC_PHYACCESS_ADDR_MAX);
+	BGMAC_BUG_ON(reg >= BGMAC_PHYACCESS_REG_MAX);
+	bgmac_maskset32(bgmac, BGMAC_PHYCTL, ~BGMAC_PHYCTL_EPA_MASK, phy);
+	/* issue the write */
+	bgmac_write32(bgmac, BGMAC_PHYACCESS, BGMAC_PHYACCESS_START |
+					      BGMAC_PHYACCESS_WRITE |
+					      BGMAC_PHYACCESS_ADDR(phy) |
+					      BGMAC_PHYACCESS_REG(reg) |
+					      value);
+	/* wait for it to complete */
+	bgmac_waitbit32(bgmac, BGMAC_PHYACCESS, BGMAC_PHYACCESS_START,
+			100, false);
+	BGMAC_WARN_ON(bgmac_read32(bgmac, BGMAC_PHYACCESS) &
+		      BGMAC_PHYACCESS_START);
+}
+
+/** phy management helpers **/
+
+/* detect phy/switch type */
+static int
+bgmac_phy_probe(struct bgmac_device *bgmac)
+{
+	if (bgmac->phy == BGMAC_PHYACCESS_ADDR_EXTPHY &&
+	    bgmac->boardflags & BCM47XX_BFL_ENETROBO) {
+		bgmac->capabilities.phy_type = BGMAC_PHYTYPE_EXTROBO;
+	} else {
+		bgmac->capabilities.phy_type = BGMAC_PHYTYPE_UNKNOWN;
+		return -EBGMAC_PHYPROBE;
+	}
+	return BGMAC_SUCCESS;
+}
+
+/** mac management helpers **/
+
+/* mac promiscous mode */
+static void
+bgmac_mac_promisc(struct bgmac_device *bgmac)
+{
+	bgmac_cmdcfg(bgmac, BGMAC_CMDCFG_PROMISC,
+		     bgmac->state.promisc ? BGMAC_CMDCFG_PROMISC : 0);
+}
+
+/* mac link speed setup */
+static void
+bgmac_mac_speed(struct bgmac_device *bgmac,
+			    enum bgmac_linkspeed speed)
+{
+	/* 1000 Mbps Half Duplex doesn't supported by bgmac */
+	if ((speed < BGMAC_LINKSPEED_10FD) ||
+	    (speed >= BGMAC_LINKSPEED_1000HD)) {
+		BGMAC_ERROR("speed setting %s is not supported.\n",
+			    bgmac_linkspeed_name(speed));
+		BGMAC_WARN_ON(1);
+		return ;
+	}
+
+	BGMAC_WARN_ON(speed == BGMAC_LINKSPEED_AUTO);
+
+	bgmac_cmdcfg(bgmac,
+		     BGMAC_CMDCFG_ES_MASK | BGMAC_CMDCFG_HD,
+		     BGMAC_LINKSPEED(speed) | BGMAC_LINKDUPLEX(speed));
+}
+
+/* set mac ethernet address */
+static void
+bgmac_mac_etheraddr(struct bgmac_device *bgmac, u8 *etheraddr)
+{
+	bgmac_write32(bgmac, BGMAC_MACADDRHI, htonl(*(u32 *)&etheraddr[0]));
+	bgmac_write32(bgmac, BGMAC_MACADDRLO, htons(*(u16 *)&etheraddr[4]));
+}
+
+/* update software-maintained counters with values from mac mib */
+static void
+bgmac_mac_mibread(struct bgmac_device *bgmac)
+{
+	struct bgmac_stats *stats = &bgmac->stats;
+	u32 *reg, *ctr;
+
+	ctr = (u32 *)&bgmac->stats.mib;
+	for (reg = (u32 *)BGMAC_MIB;
+	     reg < (u32 *)BGMAC_MIB_PAD; reg++, ctr++)
+		*ctr = bgmac_read32(bgmac, (uint)reg);
+	/* skip pad, it hurts */
+	reg++;
+	for (; reg < (u32 *)(BGMAC_MIB + BGMAC_MIB_SIZE); reg++, ctr++)
+		*ctr = bgmac_read32(bgmac, (uint)reg);
+
+	stats->txerror = stats->mib.tx.jabber_pkts + stats->mib.tx.oversize_pkts +
+			 stats->mib.tx.underruns + stats->mib.tx.excessive_cols +
+			 stats->mib.tx.late_cols + stats->txnobuf + stats->dmade +
+			 stats->dmada + stats->dmape + stats->txuflo;
+	stats->rxerror = stats->mib.rx.jabber_pkts + stats->mib.rx.oversize_pkts +
+			 stats->mib.rx.missed_pkts + stats->mib.rx.crc_align_errs +
+			 stats->mib.rx.undersize + stats->mib.rx.crc_errs +
+			 stats->mib.rx.align_errs + stats->mib.rx.symbol_errs +
+			 stats->rxnobuf + stats->rxdmauflo + stats->rxoflo + stats->rxbadlen;
+}
+
+/* reset mac mib counters */
+static void
+bgmac_mac_mibreset(struct bgmac_device *bgmac)
+{
+	u32 *reg;
+	for (reg = (u32 *)BGMAC_MIB; reg < (u32 *)BGMAC_MIB_PAD; reg++)
+		bgmac_read32(bgmac, (uint)reg);
+	/* skip pad, it hurts */
+	reg++;
+	for (; reg < (u32 *)(BGMAC_MIB + BGMAC_MIB_SIZE); reg++)
+		bgmac_read32(bgmac, (uint)reg);
+}
+
+/* mac link speed update with switch interface MII mode.
+ * Set the speed to 100BaseT Full-Duplex if switch interface
+ * is using mii/rev. mii
+ */
+static void
+bgmac_mac_miiconfig(struct bgmac_device *bgmac)
+{
+	u32 status = bgmac_read32(bgmac, BGMAC_DEVSTAT);
+	u32 mode = BGMAC_DEVSTAT_MM(status);
+
+	BGMAC_WARN_ON(mode != 2);
+
+	if (likely((mode != 0) && (mode != 1)))
+		return;
+	if (bgmac->state.speed_override == BGMAC_LINKSPEED_AUTO)
+		bgmac_mac_speed(bgmac, BGMAC_LINKSPEED_100FD);
+	else
+		bgmac_mac_speed(bgmac, bgmac->state.speed_override);
+}
+
+/* reset mac state to default
+ *  ~ enable 802.3x tx flow control, promiscous mode off, mac loopback on
+ *  ~ set initial mac address got by invars
+ *  ~ enable mib clear on read, reset mib
+ *  ~ enable MII transfers
+ *  ~ sync mac configuration mode with current MII partner interface mode
+ *  ~ enable one rx interrupt per received frame
+ *  ~ set max frame length */
+static void
+bgmac_mac_reset(struct bgmac_device *bgmac)
+{
+	bgmac_cmdcfg(bgmac,
+		BGMAC_CMDCFG_TE | BGMAC_CMDCFG_RE | BGMAC_CMDCFG_RPI |
+		BGMAC_CMDCFG_TAI | BGMAC_CMDCFG_HD | BGMAC_CMDCFG_RL |
+		BGMAC_CMDCFG_RED | BGMAC_CMDCFG_PE | BGMAC_CMDCFG_TPI |
+		BGMAC_CMDCFG_PAD_EN | BGMAC_CMDCFG_ML,
+		BGMAC_CMDCFG_PF | BGMAC_CMDCFG_ML |
+		BGMAC_CMDCFG_NLC | BGMAC_CMDCFG_CFE);
+	bgmac_mac_etheraddr(bgmac, bgmac->etheraddr);
+	bgmac_set32(bgmac, BGMAC_DEVCTL, BGMAC_DEVCTL_MROR);
+	bgmac_mac_mibreset(bgmac);
+	bgmac_set32(bgmac, BGMAC_PHYCTL, BGMAC_PHYCTL_MTE);
+	bgmac_mac_miiconfig(bgmac);
+	bgmac_write32(bgmac, BGMAC_INTRCVLAZY, 1 << BGMAC_INTRCVLAZY_FC_SHIFT);
+	/* mtu + ethernet header */
+	bgmac_write32(bgmac, BGMAC_RXMAXLEN,
+		      bgmac->net_dev->mtu + VLAN_ETH_HLEN);
+}
+
+/* mac initialization */
+static int
+bgmac_mac_init(struct bgmac_device *bgmac)
+{
+	return BGMAC_SUCCESS;
+}
+
+/* mac setup */
+static void
+bgmac_mac_setup(struct bgmac_device *bgmac)
+{
+	u32 cfg = 0;
+	u32 mask = BGMAC_CMDCFG_PROMISC |
+		   BGMAC_CMDCFG_ML |
+		   BGMAC_CMDCFG_RPI;
+	if (bgmac->state.promisc)
+		cfg |= BGMAC_CMDCFG_PROMISC;
+	if (bgmac->state.loopback)
+		cfg |= BGMAC_CMDCFG_ML;
+	if (!bgmac->state.txflowcontrol)
+		cfg |= BGMAC_CMDCFG_RPI;
+	/* Optionally, disable phy autonegotiation and force speed/duplex */
+	if (bgmac->state.speed_override != BGMAC_LINKSPEED_AUTO) {
+		BGMAC_TRACE("Forced %s speed\n",
+			bgmac_linkspeed_name(bgmac->state.speed_override));
+		cfg |= BGMAC_PHY_LINKSPEED(bgmac->state.speed_override) |
+		       BGMAC_PHY_LINKDUPLEX(bgmac->state.speed_override);
+		mask |= BGMAC_CMDCFG_ES_MASK | BGMAC_CMDCFG_HD;
+	}
+
+	bgmac_cmdcfg(bgmac, mask, cfg);
+
+	if (!bgmac->state.promisc)
+		bgmac_mac_etheraddr(bgmac, bgmac->net_dev->dev_addr);
+}
+
+/* mac enable */
+static void
+bgmac_mac_enable(struct bgmac_device *bgmac)
+{
+	u32 mode, rxqctl, bus_clock, mdp;
+	struct ssb_bus *bus = bgmac->ssb_dev->bus;
+
+	/* set RE and TE to enable the mac */
+	bgmac_set32(bgmac, BGMAC_CMDCFG, BGMAC_CMDCFG_RE | BGMAC_CMDCFG_TE);
+
+	/* WAR to not force ht clock for 47162
+	 * when bgmac is in rev. mii mode */
+	mode = BGMAC_DEVSTAT_MM(bgmac_read32(bgmac, BGMAC_DEVSTAT));
+
+	if ((bus->chip_id != 47162) || (mode != 0))
+		/* request ht clock */
+		bgmac_set32(bgmac, BGMAC_CLKCTLST, BGMAC_CLKCTLST_FORCEHT);
+	if ((bus->chip_id == 47162) && (mode == 2))
+		ssb_chipco_chipctl_maskset(&bus->chipco,
+				SSB_CHIPCO_CHIPCTL_CC1,
+				SSB_CHIPCO_CHIPCTL_CC1_RXC_DLL_BYPASS,
+				SSB_CHIPCO_CHIPCTL_CC1_RXC_DLL_BYPASS);
+
+	/* init the mac data period. the value is set according to expr
+	 * ((128ns / bus_clock) - 3). */
+	bus_clock = ssb_clockspeed(bus) / 1000000;
+	mdp = ((bus_clock * 128) / 1000) - 3;
+	rxqctl = bgmac_read32(bgmac, BGMAC_RXQCTL);
+	rxqctl &= ~BGMAC_RXQCTL_MDP_MASK;
+	rxqctl |= (mdp << BGMAC_RXQCTL_MDP_SHIFT);
+	bgmac_write32(bgmac, BGMAC_RXQCTL, rxqctl);
+}
+
+/* mac disable */
+static void
+bgmac_mac_disable(struct bgmac_device *bgmac)
+{
+	/* deassert RE and TE while in reset */
+	/* TODO: check if it could be done out of reset */
+	bgmac_cmdcfg(bgmac, BGMAC_CMDCFG_RE | BGMAC_CMDCFG_TE, 0);
+}
+
+/* mac verification */
+static int
+bgmac_mac_verify(struct bgmac_device *bgmac)
+{
+	return BGMAC_SUCCESS;
+}
+
+/** mac watchdog **/
+
+/* 1-ne second watchdog timer */
+static void
+bgmac_watchdog(unsigned long data)
+{
+	struct net_device *dev = (struct net_device *)data;
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	int err;
+
+	spin_lock(&bgmac->lock);
+
+	bgmac->watchdog_uptime++;
+
+	if (bgmac->state.reset) {
+		BGMAC_TRACE("Reset requested.\n");
+		netif_carrier_off(bgmac->net_dev);
+		goto reset;
+	}
+	if (!netif_carrier_ok(bgmac->net_dev)) {
+		if (bgmac_core_verify(bgmac) == 0) {
+			BGMAC_TRACE("Issuing carrier up.\n");
+			netif_carrier_on(bgmac->net_dev);
+			goto done;
+		}
+reset:
+		BGMAC_TRACE("resetting...\n");
+		spin_unlock(&bgmac->lock);
+		bgmac_core_disable(bgmac);
+		bgmac_core_setup(bgmac);
+		err = bgmac_core_enable(bgmac);
+		bgmac->state.reset = err;
+		return;
+	}
+
+	/* timely verify bgmac state */
+	if ((bgmac->watchdog_uptime % 50) == 0) {
+		if ((err = bgmac_core_verify(bgmac)))
+			bgmac->state.reset = true;
+			goto done;
+	}
+
+	/* Read chip mib counters occasionally */
+	if ((bgmac->watchdog_uptime % 30) == 0)
+		bgmac_mac_mibread(bgmac);
+
+done:
+	spin_unlock(&bgmac->lock);
+
+	bgmac->watchdog.expires = jiffies + HZ;
+	add_timer(&bgmac->watchdog);
+}
+
+static void
+bgmac_watchdog_enable(struct bgmac_device *bgmac)
+{
+	bgmac->watchdog.expires = jiffies + HZ;
+	add_timer(&bgmac->watchdog);
+}
+
+static void
+bgmac_watchdog_disable(struct bgmac_device *bgmac)
+{
+	del_timer_sync(&bgmac->watchdog);
+}
+
+/** mac interrupts management **/
+
+/* mac interrupts enable */
+static void
+bgmac_isr_intrson(struct bgmac_device *bgmac)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	bgmac_write32(bgmac, BGMAC_INTMASK, bgmac->intmask);
+	/* sync readback */
+	bgmac_read32(bgmac, BGMAC_INTMASK);
+	local_irq_restore(flags);
+}
+
+/* mac interrupts disable */
+static void
+bgmac_isr_intrsoff(struct bgmac_device *bgmac)
+{
+	bgmac_write32(bgmac, BGMAC_INTMASK, 0);
+	/* sync readback */
+	bgmac_read32(bgmac, BGMAC_INTMASK);
+}
+
+/* mac interrupt handler */
+static irqreturn_t bgmac_isr(int irq, void *dev_id)
+{
+	struct net_device *dev = dev_id;
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	int handled = 0;
+	u32 intstatus;
+
+	spin_lock(&bgmac->lock);
+	intstatus = bgmac_read32(bgmac, BGMAC_INTSTAT);
+	intstatus &= bgmac->intmask;
+
+	if (intstatus) {
+		/* leave if we going down */
+		if (!(atomic_read(&bgmac->up) && netif_running(dev)))
+			goto done;
+
+		handled = 1;
+
+		if (napi_schedule_prep(&bgmac->napi)) {
+			/* disable mac interrupts */
+			bgmac_write32(bgmac, BGMAC_INTMASK, 0);
+
+			bgmac->intstatus = intstatus;
+			__napi_schedule(&bgmac->napi);
+		}
+done:
+		bgmac_write32(bgmac, BGMAC_INTSTAT, intstatus);
+		/* sync readback */
+		bgmac_read32(bgmac, BGMAC_INTSTAT);
+	}
+
+	spin_unlock(&bgmac->lock);
+	return IRQ_RETVAL(handled);
+}
+
+/** dma management **/
+
+/* reset dma channels, set default flags */
+static void bgmac_dma_reset(struct bgmac_device *bgmac)
+{
+	struct ssb_dma *dma;
+	enum ssb_dma_result res;
+	int i;
+
+	BGMAC_BUG_ON(!bgmac->dma_cfg);
+
+	for (i = 0; i < BGMAC_DMA_NUM_TX; i++) {
+		dma = bgmac->dma_tx[i];
+		if ((res = ssb_dma_reset(dma))) {
+			BGMAC_TRACE("TX dma channel %s reset fail %d result\n",
+				dma->info.name, res);
+		}
+		ssb_dma_cflags(dma, 0, SSB_DMA_CTL_PE);
+	}
+
+	if ((res = ssb_dma_reset(bgmac->dma_rx))) {
+		BGMAC_TRACE("RX dma %s reset fail %d result\n",
+			bgmac->dma_rx->info.name, res);
+	}
+	ssb_dma_cflags(bgmac->dma_rx, 0, SSB_DMA_CTL_PE | SSB_DMA_CTL_RXOC);
+}
+
+/* initialize dma channel pointers, prepare channels for operating */
+static int
+bgmac_dma_init(struct bgmac_device *bgmac)
+{
+	struct ssb_dma *dma;
+	enum ssb_dma_result res;
+	int i, txi;
+
+	if ((res = ssb_dma_open(bgmac->ssb_dev, &bgmac_dma[0],
+				&bgmac->dma_cfg)))
+		return res;
+
+	for (i = 0, txi = 0; i < bgmac->dma_cfg->nr_dma; i++) {
+		dma = bgmac->dma_cfg->dma[i];
+		if (dma->info.dir == DMA_TO_DEVICE) {
+			bgmac->dma_tx[txi++] = dma;
+			res = ssb_dma_txinit(dma, &bgmac_dma_ops);
+		} else {
+			bgmac->dma_rx = dma;
+			res = ssb_dma_rxinit(dma, &bgmac_dma_ops,
+					     BGMAC_DMA_RX_BUFSIZE,
+					     sizeof(struct bgmac_dma_rxheader));
+		}
+		if (res)
+			return res;
+	}
+
+	return BGMAC_SUCCESS;
+}
+
+/* free dma stuff */
+static void
+bgmac_dma_free(struct bgmac_device *bgmac)
+{
+	int i;
+
+	ssb_dma_close(bgmac->dma_cfg);
+	bgmac->dma_cfg = NULL;
+	for (i = 0; i < BGMAC_DMA_NUM_TX; i++)
+		bgmac->dma_tx[i] = NULL;
+	bgmac->dma_rx = NULL;
+}
+
+/* disable dma */
+static void
+bgmac_dma_disable(struct bgmac_device *bgmac)
+{
+	struct ssb_dma *dma;
+	enum ssb_dma_result res;
+	int i;
+
+	for (i = 0; i < bgmac->dma_cfg->nr_dma; i++) {
+		dma = bgmac->dma_cfg->dma[i];
+		if (!(res = ssb_dma_disable(dma)))
+			continue;
+		BGMAC_TRACE("%s dma channel %s disable failed %d result\n",
+			(dma->info.dir == DMA_TO_DEVICE) ? "TX" : "RX",
+			dma->info.name, res);
+	}
+}
+
+/* enable dma */
+static int
+bgmac_dma_enable(struct bgmac_device *bgmac)
+{
+	struct ssb_dma *dma;
+	enum ssb_dma_result res;
+	int i;
+
+	for (i = 0; i < bgmac->dma_cfg->nr_dma; i++) {
+		dma = bgmac->dma_cfg->dma[i];
+		if ((res = ssb_dma_enable(dma)))
+			return res;
+	}
+	return BGMAC_SUCCESS;
+}
+
+/* verify dma */
+static int
+bgmac_dma_verify(struct bgmac_device *bgmac)
+{
+	struct ssb_dma *dma;
+	enum ssb_dma_state state;
+	enum ssb_dma_error err;
+	enum ssb_dma_result res;
+	bool show_state;
+	int i, tmp = BGMAC_SUCCESS;
+
+	for (i = 0; i < bgmac->dma_cfg->nr_dma; i++) {
+		dma = bgmac->dma_cfg->dma[i];
+		res = ssb_dma_state(dma, &state, &err);
+		show_state = res || err;
+		if (tmp == BGMAC_SUCCESS) {
+			if (res) {
+				tmp = res;
+			} else if (err) {
+				tmp = -EBGMAC_DMA;
+			} else {
+				switch (state) {
+				case SSB_DMA_STATE_ACTIVE:
+					break;
+				case SSB_DMA_STATE_IDLE:
+					if (dma->info.dir == DMA_TO_DEVICE)
+						break;
+				default:
+					tmp = -EBGMAC_DMA;
+				}
+			}
+		}
+#ifdef BGMAC_DEBUG
+		if (dma->info.dir == DMA_TO_DEVICE)
+			show_state = show_state ||
+				     (state != SSB_DMA_STATE_ACTIVE &&
+				      state != SSB_DMA_STATE_IDLE);
+		else
+			show_state = show_state ||
+				     state != SSB_DMA_STATE_ACTIVE;
+		if (res) {
+			BGMAC_TRACE("dma channel %s, state query failed"
+				    " %d result.\n",
+				    dma->info.name, res);
+		}
+		if (show_state) {
+			BGMAC_TRACE("dma channel %s, status 0x%01x,"
+				    " error 0x%01x\n",
+				    dma->info.name, state, err);
+			ssb_dma_dumpinfo(dma, true);
+		}
+#endif
+	}
+
+	return tmp;
+}
+
+/** BGMAC core management **/
+
+/* BGMAC core reset (enter some good default state) */
+static void
+bgmac_core_reset(struct bgmac_device *bgmac)
+{
+	BGMAC_TRACE("\n");
+
+	/* update software counters before reset */
+	bgmac_mac_mibread(bgmac);
+	ssb_device_enable(bgmac->ssb_dev, 0);
+	bgmac_mac_reset(bgmac);
+	bgmac_dma_reset(bgmac);
+	bgmac->intstatus = 0;
+}
+
+/* BGMAC core init (probe hw, allocate data) */
+static int
+bgmac_core_init(struct bgmac_device *bgmac)
+{
+	struct net_device *net_dev = bgmac->net_dev;
+	int err;
+
+	atomic_set(&bgmac->up, false);
+	ssb_device_enable(bgmac->ssb_dev, 0);
+	if ((err = bgmac_phy_probe(bgmac)))
+		goto out;
+	if ((err = bgmac_dma_init(bgmac)))
+		goto out;
+	if ((err = bgmac_mac_init(bgmac)))
+		goto out;
+	if (request_irq(net_dev->irq, bgmac_isr, IRQF_SHARED,
+			net_dev->name, net_dev))
+		err = -EBGMAC_INITISR;
+
+out:
+	if (err) {
+		BGMAC_TRACE("failed with %d status\n", err);
+	}
+	return err;
+}
+
+/* BGMAC core free (release allocations) */
+static void
+bgmac_core_free(struct bgmac_device *bgmac)
+{
+	bgmac_dma_free(bgmac);
+	free_irq(bgmac->net_dev->irq, bgmac->net_dev);
+}
+
+/* setup mac, phy/switch */
+static void
+bgmac_core_setup(struct bgmac_device *bgmac)
+{
+	bgmac_mac_setup(bgmac);
+}
+
+/* get stuff running */
+static int
+bgmac_core_enable(struct bgmac_device *bgmac)
+{
+	int err;
+
+	if ((err = bgmac_dma_enable(bgmac)))
+		goto err;
+	bgmac_isr_intrson(bgmac);
+
+	spin_lock(&bgmac->lock);
+	bgmac_watchdog_enable(bgmac);
+	bgmac_mac_enable(bgmac);
+	spin_unlock(&bgmac->lock);
+
+	atomic_set(&bgmac->up, true);
+	return BGMAC_SUCCESS;
+
+err:
+	bgmac_core_reset(bgmac);
+	BGMAC_TRACE("Failed with %d status", err);
+	return err;
+}
+
+static void
+bgmac_core_disable(struct bgmac_device *bgmac)
+{
+	atomic_set(&bgmac->up, false);
+	bgmac_watchdog_disable(bgmac);
+
+	spin_lock(&bgmac->lock);
+	bgmac_mac_disable(bgmac);
+	bgmac_dma_disable(bgmac);
+	bgmac_core_reset(bgmac);
+	spin_unlock(&bgmac->lock);
+
+	bgmac_isr_intrsoff(bgmac);
+}
+
+static int
+bgmac_core_verify(struct bgmac_device *bgmac)
+{
+	int err;
+
+	if ((err = bgmac_mac_verify(bgmac)))
+		goto out;
+	if ((err = bgmac_dma_verify(bgmac)))
+		goto out;
+out:
+	if (err) {
+		BGMAC_TRACE("core verification status %d\n", err);
+	}
+	return err;
+}
+
+/*** kernel interface handlers ***/
+
+/** MII interface handlers **/
+static int
+bgmac_mdio_readphy(struct net_device *dev, int phy_id, int location)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	return bgmac_phy_read16(bgmac, phy_id, location);
+}
+
+static void
+bgmac_mdio_writephy(struct net_device *dev, int phy_id, int location, int val)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	bgmac_phy_write16(bgmac, phy_id, location, val);
+}
+
+/** network device driver interface handlers **/
+
+/* process tx dma channels */
+static void
+bgmac_tx(struct bgmac_device *bgmac)
+{
+	struct ssb_dma *dma;
+	enum ssb_dma_result res;
+	u32 mask = BGMAC_INTSTAT_XI0;
+	int i;
+	for (i = 0; i < BGMAC_DMA_NUM_TX; i++, mask <<= 1) {
+		if (!(bgmac->intstatus & mask))
+			continue;
+
+		dma = bgmac->dma_tx[i];
+		if ((res = ssb_dma_queue(dma))) {
+			BGMAC_TRACE("ssb_dma_queue failed"
+				    " with %d result\n", res);
+			netif_carrier_off(bgmac->net_dev);
+		}
+	}
+}
+
+/* process rx dma channel */
+static int
+bgmac_rx(struct bgmac_device *bgmac, int budget)
+{
+	struct bgmac_dma_rxheader *rxhdr;
+	struct sk_buff *skb;
+	enum ssb_dma_result res;
+	size_t length;
+	int done;
+
+	for (done = 0; budget--; done++) {
+		res = ssb_dma_rxsingle(bgmac->dma_rx, (void *)&skb);
+		if (res) {
+			if (res == -ESSB_DMA_RXNOPENDING)
+				break;
+			BGMAC_TRACE("ssb_dma_rxsingle failed"
+				    " with %d result\n", res);
+			netif_carrier_off(bgmac->net_dev);
+			break;
+		}
+		rxhdr = (void *)skb->data;
+		if (rxhdr->flags & BGMAC_RXHFLAGS_ERRORS) {
+			dev_kfree_skb_any(skb);
+			bgmac->stats.rxerror++;
+			continue;
+		}
+		/* skip dma header, account skb length by
+		 * dma header size and 4-byte ethernet frame crc */
+		length = le16_to_cpu(rxhdr->length);
+		skb_put(skb, length + sizeof(struct bgmac_dma_rxheader) - 4);
+		skb_pull(skb, sizeof(struct bgmac_dma_rxheader));
+		skb->dev = bgmac->net_dev;
+		skb->protocol = eth_type_trans(skb, bgmac->net_dev);
+		netif_receive_skb(skb);
+
+		/* update counters */
+		bgmac->stats.rxframe++;
+		bgmac->stats.rxbyte += length;
+	}
+
+	if ((res = ssb_dma_queue(bgmac->dma_rx))) {
+		if (res == -ESSB_DMA_NOMEM) {
+			BGMAC_TRACE("RX dma channel %s queue out of memory\n",
+				bgmac->dma_rx->info.name);
+		} else {
+			BGMAC_TRACE("RX dma channel %s queue fail %d status\n",
+				bgmac->dma_rx->info.name, res);
+			netif_carrier_off(bgmac->net_dev);
+		}
+	}
+	return done;
+}
+
+/* napi poll handler */
+static int
+bgmac_poll(struct napi_struct *napi, int budget)
+{
+	struct bgmac_device *bgmac = container_of(napi, struct bgmac_device,
+						  napi);
+	int done = 0;
+
+	spin_lock(&bgmac->lock);
+
+	if (bgmac->intstatus & BGMAC_INTSTAT_ERRORS) {
+		BGMAC_TRACE("interrupt handler encountered errors"
+			    " 0x%08x intstatus\n", bgmac->intstatus);
+		netif_carrier_off(bgmac->net_dev);
+	}
+
+	if (bgmac->intstatus & BGMAC_INTSTAT_RI)
+		done = bgmac_rx(bgmac, budget);
+
+	if (bgmac->intstatus & (BGMAC_INTSTAT_XI0 | BGMAC_INTSTAT_XI1 |
+				BGMAC_INTSTAT_XI2 | BGMAC_INTSTAT_XI3))
+		bgmac_tx(bgmac);
+
+	spin_unlock(&bgmac->lock);
+
+	if (budget) {
+		napi_complete(napi);
+		bgmac_isr_intrson(bgmac);
+	}
+
+	return done;
+}
+
+static int
+bgmac_open(struct net_device *dev)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	int err;
+
+	bgmac_core_setup(bgmac);
+	if ((err = bgmac_core_enable(bgmac)))
+		return err;
+	napi_enable(&bgmac->napi);
+	netif_start_queue(dev);
+
+	return BGMAC_SUCCESS;
+}
+
+static int
+bgmac_stop(struct net_device *dev)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+
+	netif_stop_queue(dev);
+	netif_carrier_off(dev);
+	napi_disable(&bgmac->napi);
+	bgmac_core_disable(bgmac);
+
+	return BGMAC_SUCCESS;
+}
+
+static netdev_tx_t
+bgmac_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	enum ssb_dma_result res;
+	netdev_tx_t err;
+	int txi;
+
+	if (skb->len > dev->mtu + VLAN_ETH_HLEN) {
+		BGMAC_TRACE("overise packet, dropping...\n");
+		dev_kfree_skb_any(skb);
+		return NETDEV_TX_OK;
+	}
+
+	if (skb->len < BGMAC_DMA_TX_FRAMESIZE_MIN &&
+	    bgmac->ssb_dev->id.revision == 0) {
+		BGMAC_TRACE("undersize packet, trimming...\n");
+		skb_trim(skb, BGMAC_DMA_TX_FRAMESIZE_MIN);
+	}
+
+	/* choose tx channel */
+	/* TODO: atm using multiple tx channels is slower than just
+	 * sending everything through some single tx channel. It doesn't
+	 * matters which one among 4 available, just when using single TX
+	 * rather than all 4 throughoutput is about 20% higher. Dont know
+	 * yet is this some sw error of mine in bgmac/ssb dma code or
+	 * some sort of hw limitation/bug. Need more testing.
+	 * To reproduce just replace txi = 0; with
+	 * txi = bgmac->stats.txframe & (BGMAC_DMA_NUM_TX - 1);
+	 *
+	txi = skb->priority & (BGMAC_DMA_NUM_TX - 1);
+	*/
+	txi = 0;
+
+	spin_lock(&bgmac->lock);
+	res = ssb_dma_txsingle(bgmac->dma_tx[txi], skb, skb->data,
+			       skb->len, true);
+	if (res) {
+		err = NETDEV_TX_BUSY;
+		if (res == -ESSB_DMA_NOFREESLOTS) {
+			bgmac->stats.txnobuf++;
+		} else {
+			BGMAC_TRACE("ssb_dma_txsingle failed with"
+				    " %d result\n", res);
+			netif_carrier_off(bgmac->net_dev);
+		}
+	} else {
+		err = NETDEV_TX_OK;
+		/* update counters */
+		bgmac->stats.txframe++;
+		bgmac->stats.txbyte += skb->len;
+	}
+
+	spin_unlock(&bgmac->lock);
+	return err;
+}
+
+static void
+bgmac_set_multicast_list(struct net_device *dev)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+
+	spin_lock(&bgmac->lock);
+
+	if (atomic_read(&bgmac->up)) {
+		bgmac->state.allmulti = dev->flags & IFF_ALLMULTI;
+		if (bgmac->state.promisc ^ (dev->flags & IFF_PROMISC)) {
+			bgmac->state.promisc = dev->flags & IFF_PROMISC;
+			bgmac_mac_promisc(bgmac);
+		}
+
+		if (netdev_mc_count(dev))
+			bgmac->state.allmulti = true;
+	}
+
+	spin_unlock(&bgmac->lock);
+}
+
+static int
+bgmac_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+
+	BGMAC_BUG_ON(!is_valid_ether_addr(((struct sockaddr *)addr)->sa_data));
+
+	if (!atomic_read(&bgmac->up))
+		return -EBGMAC_BUSY;
+
+	spin_lock(&bgmac->lock);
+	memcpy(dev->dev_addr, ((struct sockaddr *)addr)->sa_data, dev->addr_len);
+	bgmac = netdev_priv(dev);
+	if (!bgmac->state.promisc)
+		bgmac_mac_etheraddr(bgmac, bgmac->net_dev->dev_addr);
+	spin_unlock(&bgmac->lock);
+
+	return BGMAC_SUCCESS;
+}
+
+static int
+bgmac_do_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	struct bgmac_device *bgmac;
+	int err;
+
+	bgmac = netdev_priv(dev);
+	spin_lock(&bgmac->lock);
+	err = generic_mii_ioctl(&bgmac->mii_if, if_mii(ifr), cmd, NULL);
+	spin_unlock(&bgmac->lock);
+
+	return err;
+}
+
+int bgmac_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct bgmac_device *bgmac;
+
+	if (!netif_running(dev))
+		return -EBGMAC_INVAL;
+
+	if (new_mtu < BGMAC_DMA_TX_FRAMESIZE_MIN ||
+	    new_mtu > BGMAC_DMA_RX_FRAMESIZE_MAX)
+		return -EBGMAC_INVAL;
+
+	bgmac = netdev_priv(dev);
+	spin_lock(&bgmac->lock);
+	if (atomic_read(&bgmac->up))
+		bgmac->state.reset = true;
+	bgmac->net_dev->mtu = new_mtu;
+	spin_unlock(&bgmac->lock);
+	return -EBGMAC_INVAL;
+}
+
+void bgmac_tx_timeout(struct net_device *dev)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	
+	spin_lock(&bgmac->lock);
+	if (atomic_read(&bgmac->up)) {
+		bgmac->state.reset = true;
+		BGMAC_TRACE("Transmitting path timeout, requesting reset...\n");
+	}
+	spin_unlock(&bgmac->lock);
+}
+
+struct net_device_stats* bgmac_get_stats(struct net_device *dev)
+{
+	struct bgmac_device *bgmac = netdev_priv(dev);
+	struct net_device_stats *stats = &dev->stats;
+	struct bgmac_stats *swstats = &bgmac->stats;
+	struct bgmac_mib *mib = &swstats->mib;
+
+	memset(stats, 0, sizeof(*stats));
+
+	spin_lock(&bgmac->lock);
+
+	if (atomic_read(&bgmac->up))
+		bgmac_mac_mibread(bgmac);
+
+	stats->rx_packets = swstats->rxframe;
+	stats->tx_packets = swstats->txframe;
+	stats->rx_bytes = swstats->rxbyte;
+	stats->tx_bytes = swstats->txbyte;
+	stats->rx_errors = swstats->rxerror;
+	stats->tx_errors = swstats->txerror;
+	stats->rx_over_errors = swstats->rxoflo;
+	stats->rx_fifo_errors = swstats->rxoflo;
+	stats->tx_fifo_errors = swstats->txuflo;
+
+	stats->collisions = mib->tx.total_cols;
+	stats->rx_length_errors = mib->rx.oversize_pkts + mib->rx.undersize;
+	stats->rx_over_errors = swstats->rxoflo;
+	stats->rx_frame_errors = mib->rx.align_errs;
+	stats->rx_crc_errors = mib->rx.crc_errs;
+
+	spin_unlock(&bgmac->lock);
+
+	return stats;
+}
+
+static const struct net_device_ops bgmac_netdev_ops = {
+	.ndo_open		= bgmac_open,
+	.ndo_stop		= bgmac_stop,
+	.ndo_start_xmit		= bgmac_start_xmit,
+	.ndo_set_multicast_list	= bgmac_set_multicast_list,
+	.ndo_set_mac_address	= bgmac_set_mac_address,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_do_ioctl		= bgmac_do_ioctl,
+	.ndo_change_mtu		= bgmac_change_mtu,
+	.ndo_tx_timeout		= bgmac_tx_timeout,
+	.ndo_get_stats		= bgmac_get_stats,
+};
+
+static void
+bgmac_get_invariants(struct bgmac_device *bgmac)
+{
+	struct ssb_device *dev = bgmac->ssb_dev;
+	struct ssb_bus *bus = dev->bus;
+
+	bgmac->boardflags = (bus->sprom.boardflags_hi << 16) |
+			    bus->sprom.boardflags_lo;
+	bgmac->phy = dev->bus->sprom.et0phyaddr;
+	BGMAC_BUG_ON(!is_valid_ether_addr(&bus->sprom.et0mac[0]));
+	memcpy(bgmac->etheraddr, bus->sprom.et0mac, ETH_ALEN);
+
+	bgmac->intmask = BGMAC_DEFAULT_INTMASK;
+}
+
+static void
+bgmac_remove(struct ssb_device *dev)
+{
+	struct bgmac_device *bgmac = ssb_get_drvdata(dev);
+
+	unregister_netdev(bgmac->net_dev);
+	free_netdev(bgmac->net_dev);
+	bgmac_core_free(bgmac);
+	ssb_set_drvdata(dev, NULL);
+	ssb_device_disable(dev, 0);
+}
+
+static int
+bgmac_suspend(struct ssb_device *dev, pm_message_t state)
+{
+	struct bgmac_device *bgmac = ssb_get_drvdata(dev);
+
+	bgmac_core_disable(bgmac);
+	bgmac_core_free(bgmac);
+	return BGMAC_SUCCESS;
+}
+
+static int
+bgmac_resume(struct ssb_device *dev)
+{
+	struct bgmac_device *bgmac = ssb_get_drvdata(dev);
+	int err;
+
+	if ((err = bgmac_core_init(bgmac)))
+		return err;
+	bgmac_core_reset(bgmac);
+	bgmac_core_setup(bgmac);
+	return bgmac_core_enable(bgmac);
+}
+
+static int
+bgmac_probe(struct ssb_device *dev, const struct ssb_device_id *id)
+{
+	int err;
+	struct net_device *netdev;
+	struct bgmac_device *bgmac;
+	struct mii_if_info *mii_if;
+
+	netdev = alloc_etherdev(sizeof(*bgmac));
+	if (netdev == NULL) {
+		err = -EBGMAC_NOMEM;
+		goto err;
+	}
+	SET_NETDEV_DEV(netdev, dev->dev);
+
+	bgmac = netdev_priv(netdev);
+
+	bgmac->ssb_dev = dev;
+	bgmac->net_dev = netdev;
+	spin_lock_init(&bgmac->lock);
+
+	/* set defaults */
+	bgmac->state = (struct bgmac_state) {
+		.promisc = BGMAC_DEFAULT_PROMISC,
+		.allmulti = BGMAC_DEFAULT_ALLMULTI,
+		.loopback = BGMAC_DEFAULT_LOOPBACK,
+		.txflowcontrol = BGMAC_DEFAULT_TXFLOWCONTROL,
+		.speed_override = BGMAC_DEFAULT_LINKSPEED,
+		.speed_advertise = BGMAC_DEFAULT_SPEED_ADVERTISE,
+		.need_advertise = bgmac->state.speed_override != BGMAC_LINKSPEED_AUTO,
+	};
+	BGMAC_WARN_ON(bgmac->state.need_advertise && !bgmac->state.speed_advertise);
+
+	ssb_set_drvdata(dev, bgmac);
+
+	bgmac_get_invariants(bgmac);
+
+	memcpy(netdev->dev_addr, bgmac->etheraddr, ETH_ALEN);
+	memcpy(netdev->perm_addr, netdev->dev_addr, netdev->addr_len);
+	netdev->netdev_ops = &bgmac_netdev_ops;
+	netdev->watchdog_timeo = BGMAC_TX_TIMEOUT;
+	netdev->irq = dev->irq;
+
+	if ((err = register_netdev(netdev))) {
+		err = -EBGMAC_NETDEV;
+		goto err_remove;
+	}
+	netif_carrier_off(netdev);
+
+	if ((err = bgmac_core_init(bgmac)))
+		goto err_remove;
+	bgmac_core_reset(bgmac);
+
+	init_timer(&bgmac->watchdog);
+	bgmac->watchdog.data = (u32)bgmac->net_dev;
+	bgmac->watchdog.function = bgmac_watchdog;
+
+	netif_napi_add(netdev, &bgmac->napi, bgmac_poll, 64);
+
+	mii_if = &bgmac->mii_if;
+	mii_if->dev = netdev;
+	mii_if->mdio_read = bgmac_mdio_readphy;
+	mii_if->mdio_write = bgmac_mdio_writephy;
+	mii_if->phy_id = bgmac->phy;
+	mii_if->phy_id_mask = BGMAC_PHYACCESS_ADDR_MII_MASK;
+	mii_if->reg_num_mask = BGMAC_PHYACCESS_REG_MII_MASK;
+
+	netdev_info(netdev, BGMAC_MODULE_DESCRIPTION " (%s) %pM\n",
+		    bgmac_phy_name(bgmac->capabilities.phy_type),
+		    netdev->dev_addr);
+
+	return BGMAC_SUCCESS;
+err_remove:
+	bgmac_remove(dev);
+err:
+	BGMAC_ERROR("Failed with %d status\n", err);
+	return err;
+}
+
+static struct ssb_driver bgmac_driver = {
+	.name		= BGMAC_MODULE_NAME,
+	.id_table	= bgmac_tbl,
+	.probe		= bgmac_probe,
+	.remove		= bgmac_remove,
+	.suspend	= bgmac_suspend,
+	.resume		= bgmac_resume,
+};
+
+static int __init
+bgmac_driver_init(void)
+{
+        return ssb_driver_register(&bgmac_driver);
+}
+
+static void __exit
+bgmac_driver_exit(void)
+{
+	ssb_driver_unregister(&bgmac_driver);
+}
+
+module_init(bgmac_driver_init);
+module_exit(bgmac_driver_exit);
--- linux-3.0.4.orig/drivers/net/bgmac.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-3.0.4/drivers/net/bgmac.h	2011-09-25 00:05:45.000000000 +0300
@@ -0,0 +1,506 @@
+#ifndef LINUX_DRIVER_BGMAC_H_
+#define LINUX_DRIVER_BGMAC_H_
+
+#define PFX "bgmac: "
+
+#define bgmac_printk(level, fmt, args...) printk(level PFX fmt, ## args);
+#ifdef BGMAC_DEBUG
+# define bgmac_dprintk(level, fmt, args...) printk(level PFX "%s: " fmt, __FUNCTION__, ## args);
+# define BGMAC_TRACE(fmt, args...) bgmac_dprintk(KERN_INFO, fmt, ## args);
+# define BGMAC_INFO(fmt, args...) bgmac_dprintk(KERN_INFO, fmt, ## args);
+# define BGMAC_WARN(fmt, args...) bgmac_dprintk(KERN_WARNING, fmt, ## args);
+# define BGMAC_ERROR(fmt, args...) bgmac_dprintk(KERN_ERR, fmt, ## args);
+# define BGMAC_WARN_ON(x) WARN_ON(x)
+# define BGMAC_BUG_ON(x) do { if (x) BGMAC_ERROR("BUG .%d\n", __LINE__); BUG_ON(x); } while(0)
+#else
+# define BGMAC_INFO(fmt, args...) bgmac_printk(KERN_INFO, fmt, ## args);
+# define BGMAC_WARN(fmt, args...) bgmac_printk(KERN_WARNING, fmt, ## args);
+# define BGMAC_ERROR(fmt, args...) bgmac_printk(KERN_ERR, fmt, ## args);
+# define BGMAC_TRACE(fmt, args...) do {} while(0)
+# define BGMAC_WARN_ON(x) do {} while(0)
+# define BGMAC_BUG_ON(x) do {} while(0)
+#endif
+
+/* We have 4 DMA channels */
+#define BGMAC_DMA_NUM_TX		4
+/* number of dma tx ring slots (must be power of 2) */
+#define BGMAC_DMA_TX_SLOTS		64
+/* number of dma rx ring slots (must be power of 2) */
+#define BGMAC_DMA_RX_SLOTS		128
+/* size of dma rx buffer */
+#define BGMAC_DMA_RX_BUFSIZE		(VLAN_ETH_FRAME_LEN + VLAN_ETH_HLEN + \
+					 sizeof(struct bgmac_dma_rxheader))
+/* max rx data payload */
+#define BGMAC_DMA_RX_FRAMESIZE_MAX	VLAN_ETH_FRAME_LEN
+/* gmac can only send frames of size above 17 octets */
+#define BGMAC_DMA_TX_FRAMESIZE_MIN	17
+
+/* defaults */
+#define BGMAC_DEFAULT_PROMISC		false
+#define BGMAC_DEFAULT_ALLMULTI		false
+#define BGMAC_DEFAULT_LOOPBACK		false
+#define BGMAC_DEFAULT_TXFLOWCONTROL	true
+#define BGMAC_DEFAULT_LINKSPEED		BGMAC_LINKSPEED_AUTO
+/* By default, advertise all link speeds/duplex settings
+ * except 1000 Mbps Half-Duplex. */
+#define BGMAC_DEFAULT_SPEED_ADVERTISE	(BGMAC_ADVERTISE_SPEED_ALL & \
+					 ~BGMAC_ADVERTISE_SPEED_1000HD)
+
+/* timeouts and delays settings */
+#define BGMAC_TX_TIMEOUT		(5 * HZ)
+#define BGMAC_MAC_SWRESET_DELAY		2
+#define BGMAC_PHY_RESET_DELAY		100
+
+/* board flags */
+#define BCM47XX_BFL_ENETROBO		0x00000010 /* Board has robo switch or core */
+
+/* device control register */
+#define BGMAC_DEVCTL			0x000
+#define  BGMAC_DEVCTL_TSM		0x00000002
+#define  BGMAC_DEVCTL_CFCO		0x00000004
+#define  BGMAC_DEVCTL_RLSS		0x00000008
+#define  BGMAC_DEVCTL_MROR		0x00000010 /* mib reset on read. 0 off, 1 on */
+#define  BGMAC_DEVCTL_FCM_MASK		0x00000060
+#define  BGMAC_DEVCTL_FCM_SHIFT		5
+#define  BGMAC_DEVCTL_NAE		0x00000080
+#define  BGMAC_DEVCTL_TF		0x00000100
+#define  BGMAC_DEVCTL_RDS_MASK		0x00030000
+#define  BGMAC_DEVCTL_RDS_SHIFT		16
+#define  BGMAC_DEVCTL_TDS_MASK		0x000C0000
+#define  BGMAC_DEVCTL_TDS_SHIFT		18
+
+/* device status register */
+#define BGMAC_DEVSTAT			0x0004
+#define  BGMAC_DEVSTAT_RBF		0x00000001
+#define  BGMAC_DEVSTAT_RDF		0x00000002
+#define  BGMAC_DEVSTAT_RIF		0x00000004
+#define  BGMAC_DEVSTAT_TBF		0x00000008
+#define  BGMAC_DEVSTAT_TDF		0x00000010
+#define  BGMAC_DEVSTAT_TIF		0x00000020
+#define  BGMAC_DEVSTAT_PO		0x00000040
+#define  BGMAC_DEVSTAT_MM_MASK		0x00000300 /* ?switch interface? MII mode mask */
+#define  BGMAC_DEVSTAT_MM_SHIFT		8          /* ?switch interface? MII mode shift */
+#define   BGMAC_DEVSTAT_MM_REVMII	0x0        /* ?switch interface? reverse MII (phy) mode */
+#define   BGMAC_DEVSTAT_MM_MII		0x1        /* ?switch interface? MII (mac) mode */
+#define  BGMAC_DEVSTAT_MM(ds) (((ds) & BGMAC_DEVSTAT_MM_MASK) >> \
+				BGMAC_DEVSTAT_MM_SHIFT)
+
+/* interrupt status register */
+#define BGMAC_INTSTAT			0x020
+#define  BGMAC_INTSTAT_MRO		0x00000001
+#define  BGMAC_INTSTAT_MTO		0x00000002
+#define  BGMAC_INTSTAT_TFD		0x00000004
+#define  BGMAC_INTSTAT_LS		0x00000008
+#define  BGMAC_INTSTAT_MDIO		0x00000010
+#define  BGMAC_INTSTAT_MR		0x00000020
+#define  BGMAC_INTSTAT_MT		0x00000040
+#define  BGMAC_INTSTAT_TO		0x00000080
+#define  BGMAC_INTSTAT_PDEE		0x00000400 /* descriptor error */
+#define  BGMAC_INTSTAT_PDE		0x00000800 /* data error */
+#define  BGMAC_INTSTAT_DE		0x00001000 /* descriptor protocol error */
+#define  BGMAC_INTSTAT_RDU		0x00002000 /* receive descriptor underflow */
+#define  BGMAC_INTSTAT_RFO		0x00004000 /* receive fifo overflow */
+#define  BGMAC_INTSTAT_XFU		0x00008000 /* transmit fifo underflow */
+#define  BGMAC_INTSTAT_RI		0x00010000
+#define  BGMAC_INTSTAT_XI0		0x01000000
+#define  BGMAC_INTSTAT_XI1		0x02000000
+#define  BGMAC_INTSTAT_XI2		0x04000000
+#define  BGMAC_INTSTAT_XI3		0x08000000
+#define  BGMAC_INTSTAT_ERRORS		(BGMAC_INTSTAT_PDEE | \
+					 BGMAC_INTSTAT_PDE | \
+					 BGMAC_INTSTAT_DE | \
+					 BGMAC_INTSTAT_RDU | \
+					 BGMAC_INTSTAT_RFO | \
+					 BGMAC_INTSTAT_XFU)
+
+/* interrupts mask register */
+#define BGMAC_INTMASK			0x024
+#define BGMAC_DEFAULT_INTMASK		(BGMAC_INTSTAT_XI0 | \
+					 BGMAC_INTSTAT_XI1 | \
+					 BGMAC_INTSTAT_XI2 | \
+					 BGMAC_INTSTAT_XI3 | \
+					 BGMAC_INTSTAT_RI | \
+					 BGMAC_INTSTAT_ERRORS)
+
+/* rx interrupt control register */
+#define BGMAC_INTRCVLAZY		0x100
+#define  BGMAC_INTRCVLAZY_TO_MASK	0x00FFFFFF
+#define  BGMAC_INTRCVLAZY_FC_MASK	0xFF000000 /* ?frame control? "1" means one rx interrupt per received frame */
+#define  BGMAC_INTRCVLAZY_FC_SHIFT	24
+
+/* phy access register */
+#define BGMAC_PHYACCESS			0x180
+#define  BGMAC_PHYACCESS_DATA_MASK	0x0000FFFF /* data mask */
+#define  BGMAC_PHYACCESS_ADDR_MASK	0x001F0000 /* phy address mask */
+#define  BGMAC_PHYACCESS_ADDR_SHIFT	16         /* phy address shift */
+#define  BGMAC_PHYACCESS_ADDR_MAX	32         /* max 32 phy */
+#define  BGMAC_PHYACCESS_ADDR_EXTPHY	30         /* external phy/switch */
+#define  BGMAC_PHYACCESS_ADDR_PSEUDOPHY	0x1E       /* MII Pseudo PHY address */
+#define  BGMAC_PHYACCESS_ADDR_MII_MASK	0x1F       /* phy address mask for MII ioctls */
+#define  BGMAC_PHYACCESS_REG_MASK	0x1F000000 /* phy register mask */
+#define  BGMAC_PHYACCESS_REG_SHIFT	24         /* phy register shift */
+#define  BGMAC_PHYACCESS_REG_MAX	32         /* max 32 registers per phy */
+#define  BGMAC_PHYACCESS_REG_MII_MASK	0x1F       /* register mask for MII ioctls */
+#define  BGMAC_PHYACCESS_WRITE		0x20000000 /* operation. 0 read, 1 write */
+#define  BGMAC_PHYACCESS_START		0x40000000 /* access operation state. 0 done, 1 pending */
+/* phyaccess reg/addr access helper macro defs */
+#define  BGMAC_PHYACCESS_ADDR(addr) ((addr) << BGMAC_PHYACCESS_ADDR_SHIFT)
+#define  BGMAC_PHYACCESS_REG(reg) ((reg) << BGMAC_PHYACCESS_REG_SHIFT)
+
+/* phy control register */
+#define BGMAC_PHYCTL			0x188
+#define  BGMAC_PHYCTL_EPA_MASK		0x0000001F /* phy access address mask */
+#define  BGMAC_PHYCTL_MCT_MASK		0x007F0000
+#define  BGMAC_PHYCTL_MCT_SHIFT		16
+#define  BGMAC_PHYCTL_MTE		0x00800000 /* MII transfer enable */
+
+/* rx queue control register */
+#define BGMAC_RXQCTL			0x190
+#define  BGMAC_RXQCTL_DBT_MASK		0x00000FFF
+#define  BGMAC_RXQCTL_DBT_SHIFT		0
+#define  BGMAC_RXQCTL_PTE		0x00001000
+#define  BGMAC_RXQCTL_MDP_MASK		0x3F000000 /* mac data period mask */
+#define  BGMAC_RXQCTL_MDP_SHIFT		24         /* mac data period shift */
+
+/* pmu clock control and status register */
+#define	BGMAC_CLKCTLST			0x1E0
+#define  BGMAC_CLKCTLST_FORCEALP	0x00000001 /* Force ALP request */
+#define  BGMAC_CLKCTLST_FORCEHT		0x00000002 /* Force HT request */
+#define  BGMAC_CLKCTLST_FORCEILP	0x00000004 /* Force ILP request */
+#define  BGMAC_CLKCTLST_HAVEALPREQ	0x00000008 /* ALP available request */
+#define  BGMAC_CLKCTLST_HAVEHTREQ	0x00000010 /* HT available request */
+#define  BGMAC_CLKCTLST_HWCROFF		0x00000020 /* Force HW clock request off */
+#define  BGMAC_CLKCTLST_ERREQ		0x00000100 /* external resource request */
+#define  BGMAC_CLKCTLST_HAVEALP		0x00010000 /* ALP is available */
+#define  BGMAC_CLKCTLST_HAVEHT		0x00020000 /* HT is available */
+#define  BGMAC_CLKCTLST_ALP		0x00040000 /* RO: backplane is running on ALP clock */
+#define  BGMAC_CLKCTLST_HT		0x00080000 /* RO: Backplane is running on HT clock */
+#define  BGMAC_CLKCTLST_ERSTAT		0x01000000 /* external resource status mask */
+
+/* tx/rx dma channel registers base */
+#define BGMAC_DMA0_TX			0x200
+#define BGMAC_DMA0_RX			(BGMAC_DMA0_TX + 0x020)
+#define BGMAC_DMA(d,i) (BGMAC_DMA0_##d + (i) * 0x040)
+
+/* mib counters registers */
+#define BGMAC_MIB			0x300
+#define BGMAC_MIB_PADIDX		43         /* pad */
+#define BGMAC_MIB_PAD			(BGMAC_MIB + BGMAC_MIB_PADIDX * 4)
+#define BGMAC_MIB_NRCOUNTERS		75         /* whole number of counters in mib including pad */
+#define BGMAC_MIB_SIZE			(BGMAC_MIB_NRCOUNTERS * 4)
+
+/* command config register */
+#define BGMAC_CMDCFG			0x808
+#define  BGMAC_CMDCFG_TE		0x00000001 /* tx enable */
+#define  BGMAC_CMDCFG_RE		0x00000002 /* rx enable */
+#define  BGMAC_CMDCFG_ES_MASK		0x0000000C /* enet link speed mask */
+#define  BGMAC_CMDCFG_ES_SHIFT		2          /* enet link speed shift */
+#define   BGMAC_CMDCFG_ES_10		0x00000000 /* 10 Mbps, 0 << 2 */
+#define   BGMAC_CMDCFG_ES_100		0x00000004 /* 100 Mbps, 1 << 2 */
+#define   BGMAC_CMDCFG_ES_1000		0x00000008 /* 1000 Mbps, 2 << 2 */
+#define  BGMAC_CMDCFG_PROMISC		0x00000010 /* promiscous mode. 0 off, 1 on */
+#define  BGMAC_CMDCFG_PAD_EN		0x00000020
+#define  BGMAC_CMDCFG_CF		0x00000040
+#define  BGMAC_CMDCFG_PF		0x00000080
+#define  BGMAC_CMDCFG_RPI		0x00000100 /* 0 tx flow control enable, 1 rx pause ignore */
+#define  BGMAC_CMDCFG_TAI		0x00000200
+#define  BGMAC_CMDCFG_HD		0x00000400 /* half duplex. 0 off, 1 on */
+#define  BGMAC_CMDCFG_HD_SHIFT		10
+#define  BGMAC_CMDCFG_SR		0x00000800 /* soft reset. 0 on, 1 off */
+#define  BGMAC_CMDCFG_ML		0x00008000 /* mac loopback mode. 0 off, 1 on */
+#define  BGMAC_CMDCFG_AE		0x00400000
+#define  BGMAC_CMDCFG_CFE		0x00800000
+#define  BGMAC_CMDCFG_NLC		0x01000000
+#define  BGMAC_CMDCFG_RL		0x02000000
+#define  BGMAC_CMDCFG_RED		0x04000000
+#define  BGMAC_CMDCFG_PE		0x08000000
+#define  BGMAC_CMDCFG_TPI		0x10000000
+#define  BGMAC_CMDCFG_AT		0x20000000
+#define  BGMAC_CMDCFG_ES(cfg) (((cfg) & BGMAC_CMDCFG_ES_MASK) >> BGMAC_CMDCFG_ES_SHIFT)
+
+/* mac address (high 4 bytes), big endian */
+#define BGMAC_MACADDRHI			0x80C
+
+/* mac address (low 2 bytes), big endian */
+#define BGMAC_MACADDRLO			0x810
+
+/* rx max frame length control register */
+#define BGMAC_RXMAXLEN			0x814
+
+/* single transmission queue counters */
+struct bgmac_mib_queue {
+	u32 pkts;
+	u32 octets;
+	u32 octets_high;
+} __packet;
+
+#define BGMAC_MIB_REG(reg) u32 reg;
+#define BGMAC_MIB_REGS_DECLARE			\
+		BGMAC_MIB_REG(good_octets)	\
+		BGMAC_MIB_REG(good_octets_hi)	\
+		BGMAC_MIB_REG(good_pkts)	\
+		BGMAC_MIB_REG(octets)		\
+		BGMAC_MIB_REG(octets_hi)	\
+		BGMAC_MIB_REG(pkts)		\
+		BGMAC_MIB_REG(broadcast_pkts)	\
+		BGMAC_MIB_REG(multicast_pkts)	\
+		BGMAC_MIB_REG(len_64)		\
+		BGMAC_MIB_REG(len_65_to_127)	\
+		BGMAC_MIB_REG(len_128_to_255)	\
+		BGMAC_MIB_REG(len_256_to_511)	\
+		BGMAC_MIB_REG(len_512_to_1023)	\
+		BGMAC_MIB_REG(len_1024_to_1522)	\
+		BGMAC_MIB_REG(len_1523_to_2047)	\
+		BGMAC_MIB_REG(len_2048_to_4095)	\
+		BGMAC_MIB_REG(len_4095_to_8191)	\
+		BGMAC_MIB_REG(len_8192_to_max)	\
+		BGMAC_MIB_REG(jabber_pkts)	\
+		BGMAC_MIB_REG(oversize_pkts)	\
+		BGMAC_MIB_REG(fragment_pkts)
+
+/* transmission counters */
+struct bgmac_mib_tx {
+	BGMAC_MIB_REGS_DECLARE
+	u32 underruns;
+	u32 total_cols;
+	u32 single_cols;
+	u32 multiple_cols;
+	u32 excessive_cols;
+	u32 late_cols;
+	u32 defered;
+	u32 carrier_lost;
+	u32 pause_pkts;
+	u32 uni_pkts;
+} __packed;
+
+/* reception counters */
+struct bgmac_mib_rx {
+	BGMAC_MIB_REGS_DECLARE
+	u32 missed_pkts;
+	u32 crc_align_errs;
+	u32 undersize;
+	u32 crc_errs;
+	u32 align_errs;
+	u32 symbol_errs;
+	u32 pause_pkts;
+	u32 nonpause_pkts;
+	u32 sachanges;
+	u32 uni_pkts;
+} __packed;
+
+/* BGMAC MIB counters */
+struct bgmac_mib {
+	struct bgmac_mib_tx tx;
+	struct bgmac_mib_queue txq[4];
+
+	/* here is pad in hw mib registers' space */
+
+	struct bgmac_mib_rx rx;
+} __packed;
+
+/* bgmac error codes */
+enum bgmac_error {
+    BGMAC_SUCCESS,				/* success status */
+    EBGMAC_NOMEM = ENOMEM,			/* out of memory */
+    EBGMAC_BUSY = EBUSY,			/* busy */
+    EBGMAC_INVAL = EINVAL,			/* invalid argument */
+    EBGMAC_ERROR = 0x0100,			/* common error */
+    EBGMAC_INVARS = EBGMAC_ERROR + 1,		/* chip invariants */
+    EBGMAC_NETDEV = EBGMAC_ERROR + 2,		/* netdevice related */
+    EBGMAC_PROBE = EBGMAC_ERROR + 3,		/* brobe failure */
+    EBGMAC_INITISR = EBGMAC_ERROR + 4,		/* isr initialization failure */
+    EBGMAC_PHYRESET = EBGMAC_ERROR + 5,		/* phy reset failure */
+    EBGMAC_PHYPROBE = EBGMAC_ERROR + 6,		/* failure while probing phy */
+    EBGMAC_PHYUNKNOWN = EBGMAC_ERROR + 7,	/* unknown or unsupported phy */
+    EBGMAC_DMAINIT = EBGMAC_ERROR + 8,		/* failed to initialize dma */
+    EBGMAC_DMA = EBGMAC_ERROR + 9,		/* dma engine failure */
+};
+
+enum bgmac_linkspeed {
+    BGMAC_LINKSPEED_AUTO = -1,
+    BGMAC_LINKSPEED_10FD = 0,
+    BGMAC_LINKSPEED_10HD,
+    BGMAC_LINKSPEED_100FD,
+    BGMAC_LINKSPEED_100HD,
+    BGMAC_LINKSPEED_1000FD,
+    BGMAC_LINKSPEED_1000HD,
+    BGMAC_LINKSPEED_MAX,
+};
+
+/* bgmac_linkspeed enum to BGMAC_CMDCFG speed/duplex conversion macro defs */
+#define BGMAC_LINKSPEED(speed) (((u32)(speed) & 0x06) << \
+				(BGMAC_CMDCFG_ES_SHIFT - 1))
+#define BGMAC_LINKDUPLEX(speed) (((u32)(speed) & 0x01) << \
+				 BGMAC_CMDCFG_HD_SHIFT)
+
+/* BGMAC_CMDCFG_ES/BGMAC_CMDCFG_HD to bgmac_linkspeed enum conversion macro defs */
+#define BGMAC_CMDCFG_ES_LINKSPEED(cfg) ((BGMAC_CMDCFG_ES(cfg) << 1) | \
+					!!((cfg) & BGMAC_CMDCFG_HD))
+
+/* bgmac_linkspeed enum to BGMAC_PHY_CONTROL speed/duplex conversion macro defs */
+#define BGMAC_PHY_LINKSPEED(speed) ((((u16)(speed) & 0x02) << 12) | \
+				    (((u16)(speed) & 0x04) << 4))
+#define BGMAC_PHY_LINKDUPLEX(speed) (!((u16)(speed) & 0x01) << 8)
+
+enum bgmac_txqueue_id {
+    BGMAC_TXQUEUE_0,
+    BGMAC_TXQUEUE_1,
+    BGMAC_TXQUEUE_2,
+    BGMAC_TXQUEUE_3,
+    BGMAC_TXQUEUE_MAX = BGMAC_TXQUEUE_3,
+};
+
+/* software-maintained counters */
+struct bgmac_stats {
+	u32 txerror;	/* total tx errors */
+	u32 rxerror;	/* total rx errors */
+#if 0
+	u32 rxgiants;	/* total rx giant frames */
+#endif
+	u32 dmade;	/* dma descriptor errors */
+	u32 dmada;	/* dma data errors */
+	u32 dmape;	/* descriptor protocol error */
+	u32 txnobuf;	/* tx out-of-buffer errors */
+	u32 txuflo;	/* transmit fifo underflow */
+	u32 txbyte;	/* transmitted bytes */
+	u32 txframe;	/* transmitted frames */
+	u32 rxbyte;	/* received bytes */
+	u32 rxnobuf;	/* rx out-of-buffer errors */
+	u32 rxframe;	/* received frames */
+	u32 rxdmauflo;	/* receive descriptor underflow */
+	u32 rxoflo;	/* receive fifo overflow */
+	u32 rxbadlen;	/* 802.3 len field != read length */
+	/* Software-maintained MIB copy */
+	struct bgmac_mib mib;
+};
+
+/* bgmac autonegotiation advertisement speeds */
+#define BGMAC_ADVERTISE_SPEED_10HD	(1 << BGMAC_LINKSPEED_10FD)
+#define BGMAC_ADVERTISE_SPEED_10FD	(1 << BGMAC_LINKSPEED_10HD)
+#define BGMAC_ADVERTISE_SPEED_100HD	(1 << BGMAC_LINKSPEED_100FD)
+#define BGMAC_ADVERTISE_SPEED_100FD	(1 << BGMAC_LINKSPEED_100HD)
+#define BGMAC_ADVERTISE_SPEED_1000HD	(1 << BGMAC_LINKSPEED_1000FD)
+#define BGMAC_ADVERTISE_SPEED_1000FD	(1 << BGMAC_LINKSPEED_1000HD)
+#define BGMAC_ADVERTISE_SPEED_ALL	(BGMAC_ADVERTISE_SPEED_10HD | \
+					 BGMAC_ADVERTISE_SPEED_10FD | \
+					 BGMAC_ADVERTISE_SPEED_100HD | \
+					 BGMAC_ADVERTISE_SPEED_100FD | \
+					 BGMAC_ADVERTISE_SPEED_1000HD | \
+					 BGMAC_ADVERTISE_SPEED_1000FD)
+
+/* bgmac state bit field */
+struct bgmac_state {
+	bool reset: 1;		/* reset required */
+	bool promisc: 1;	/* promiscous mode */
+	bool allmulti: 1;	/* multicast promiscous mode */
+	bool loopback: 1;	/* loopback mode */
+	bool txflowcontrol: 1;	/* tx flow control */
+	/* link speed autonegotiation restart requested */
+	bool need_advertise: 1;
+	/* actual link speed */
+	enum bgmac_linkspeed linkspeed: 4;
+	/* disable link speed autonegotiation and force speed/duplex
+	 * if other than BGMAC_SPEED_AUTO */
+	enum bgmac_linkspeed speed_override: 4;
+	/* autonegotiation advertised link speeds
+	 * (see BGMAC_ADVERTISE_SPEED_(10|100|1000)(F|H)D) */
+	unsigned short speed_advertise: 6;
+};
+
+/* bgmac phy type */
+enum bgmac_phy_type {
+	BGMAC_PHYTYPE_UNKNOWN = -1,	/* some unknown or unsupported phy */
+	BGMAC_PHYTYPE_EXTROBO,		/* external robo switch */
+	BGMAC_PHYTYPE_MAX,
+};
+
+/* bgmac capabilities bit field */
+struct bgmac_capabilities {
+	enum bgmac_phy_type phy_type: 2;
+};
+
+/* bgmac device */
+struct bgmac_device {
+	struct ssb_device *ssb_dev;
+	struct net_device *net_dev;
+
+	/* software-maintained counters */
+	struct bgmac_stats stats;
+
+	spinlock_t lock;
+
+	/* MII interface */
+	struct mii_if_info mii_if;
+	/* napi interface */
+	struct napi_struct napi;
+
+	/* MII phy address */
+	u8 phy;
+
+	/* board flags */
+	u32 boardflags;
+
+	/* initial ethernet address */
+	u8 etheraddr[ETH_ALEN];
+
+	/* number of this bgmac. 0 by now and
+	 * might be to the end of days */
+	u8 unit;
+
+	/* watchdog timer */
+	struct timer_list watchdog;
+	/* watchdog timer uptime in seconds */
+	u32 watchdog_uptime;
+
+	struct ssb_dma_config *dma_cfg; /* dma configuration data */
+	/* tx dma channels */
+	struct ssb_dma *dma_tx[BGMAC_DMA_NUM_TX];
+	/* rx dma channel */
+	struct ssb_dma *dma_rx;
+
+	/*  */
+	u32 intmask;
+	/*  */
+	u32 intstatus;
+
+	/* bgmac capabilities */
+	struct bgmac_capabilities capabilities;
+
+	/* bgmac operating state */
+	atomic_t up;
+	/* bgmac state record */
+	struct bgmac_state state;
+};
+
+#define BGMAC_RXHLEN			0x00
+#define BGMAC_RXHFLAGS			0x02
+#define  BGMAC_RXHFLAGS_DT_MASK		0xF000	/* data type */
+#define  BGMAC_RXHFLAGS_DT_SHIFT	12
+#define  BGMAC_RXHFLAGS_DC_MASK		0x0F00	/* (num descr to xfer the frame) - 1 */
+#define  BGMAC_RXHFLAGS_DC_SHIFT	8
+#define  BGMAC_RXHFLAGS_PT_MASK		0x0003	/* packet type. 0 - Unicast,
+						 * 1 - Multicast, 2 - Broadcast */
+#define  BGMAC_RXHFLAGS_VLAN		0x0004	/* vlan tag detected */
+#define  BGMAC_RXHFLAGS_CRC		0x0008	/* crc error */
+#define  BGMAC_RXHFLAGS_OVERSIZE	0x0010	/* frame size > rxmaxlength */
+#define  BGMAC_RXHFLAGS_OVF		0x0080	/* overflow error occured */
+#define  BGMAC_RXHFLAGS_ERRORS		(BGMAC_RXHFLAGS_CRC | \
+					 BGMAC_RXHFLAGS_OVERSIZE | \
+					 BGMAC_RXHFLAGS_OVF)
+
+#define BGMAC_RXHFLAGS_DC(f) (((f) & BGMAC_RXHFLAGS_DC_MASK) >> \
+				     BGMAC_RXHFLAGS_DC_SHIFT)
+#define BGMAC_RXHFLAGS_DT(f) (((f) & BGMAC_RXHFLAGS_DT_MASK) >> \
+				     BGMAC_RXHFLAGS_DT_SHIFT)
+
+/* rx header prepended by gmac core dma engine */
+struct bgmac_dma_rxheader {
+	__le16 length;
+	__le16 flags;
+	__le16 pad[13];
+} __packed;
+
+extern u16 bgmac_phy_read16(struct bgmac_device *bgmac, u8 phy, u8 reg);
+extern void bgmac_phy_write16(struct bgmac_device *bgmac, u8 phy, u8 reg, u16 value);
+
+#endif /* !LINUX_DRIVER_BGMAC_H_ */
--- linux-3.0.4.orig/drivers/net/Kconfig	2011-09-25 00:01:33.000000000 +0300
+++ linux-3.0.4/drivers/net/Kconfig	2011-09-25 00:05:45.000000000 +0300
@@ -2561,6 +2561,14 @@ config PCH_GBE
 	  ML7223 is companion chip for Intel Atom E6xx series.
 	  ML7223 is completely compatible for Intel EG20T PCH.
 
+config BGMAC
+	tristate "Broadcom SSB GMAC ethernet driver"
+	depends on BCM47XX && SSB_DMA64 && EXPERIMENTAL
+	help
+	  Driver for Broadcom SSB GMAC gigabit ethernet network interface mac.
+	  To compile this driver as module, choose M: the module will be
+	  called bgmac.
+
 endif # NETDEV_1000
 
 #
--- linux-3.0.4.orig/drivers/net/Makefile	2011-09-25 00:01:41.000000000 +0300
+++ linux-3.0.4/drivers/net/Makefile	2011-09-25 00:05:45.000000000 +0300
@@ -149,6 +149,7 @@ obj-$(CONFIG_NE_H8300) += ne-h8300.o
 obj-$(CONFIG_AX88796) += ax88796.o
 obj-$(CONFIG_BCM63XX_ENET) += bcm63xx_enet.o
 obj-$(CONFIG_FTMAC100) += ftmac100.o
+obj-$(CONFIG_BGMAC) += bgmac.o
 
 obj-$(CONFIG_TSI108_ETH) += tsi108_eth.o
 obj-$(CONFIG_MV643XX_ETH) += mv643xx_eth.o
--- linux-3.0.4.orig/drivers/ssb/Kconfig	2011-09-25 00:03:34.000000000 +0300
+++ linux-3.0.4/drivers/ssb/Kconfig	2011-09-25 00:05:45.000000000 +0300
@@ -39,6 +39,7 @@ config SSB_BUS_AI_POSSIBLE
 config SSB_BUS_AI
 	bool "AI style bus"
 	depends on SSB_BUS_AI_POSSIBLE
+	select SSB_DMA64
 	default n
 	help
 	  Sonics Silicon Backplane AI-style bus
@@ -124,6 +125,33 @@ config SSB_DEBUG
 
 	  If unsure, say N
 
+config SSB_DMA
+	bool "SSB Broadcom DMA library"
+	depends on SSB && EXPERIMENTAL
+	help
+	  Interface library for Sonics Silicon Backplane dma
+	  controllers unified access. If you dont plan to use
+	  bgmac network interface mac driver say N here.
+
+	  If unsure, say N
+
+config SSB_DMA_DEBUG
+	bool "SSB DMA library debugging"
+	depends on SSB_DMA
+	default y
+	help
+	  ssb dma debugging.
+
+config SSB_DMA32
+	bool "32-bit SSB DMA cores support"
+	depends on SSB_DMA
+	default y
+
+config SSB_DMA64
+	bool "64-bit SSB DMA cores support"
+	depends on SSB_DMA
+	default y
+
 config SSB_SERIAL
 	bool
 	depends on SSB
--- linux-3.0.4.orig/drivers/ssb/Makefile	2011-09-25 00:04:34.000000000 +0300
+++ linux-3.0.4/drivers/ssb/Makefile	2011-09-25 00:05:45.000000000 +0300
@@ -18,6 +18,7 @@ ssb-$(CONFIG_SSB_DRIVER_MIPS)		+= driver
 ssb-$(CONFIG_SSB_DRIVER_EXTIF)		+= driver_extif.o
 ssb-$(CONFIG_SSB_DRIVER_PCICORE)	+= driver_pcicore.o
 ssb-$(CONFIG_SSB_DRIVER_GIGE)		+= driver_gige.o
+ssb-$(CONFIG_SSB_DMA)			+= ssb_dma.o
 
 # b43 pci-ssb-bridge driver
 # Not strictly a part of SSB, but kept here for convenience
--- linux-3.0.4.orig/drivers/ssb/ssb_dma.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-3.0.4/drivers/ssb/ssb_dma.c	2011-09-25 00:05:45.000000000 +0300
@@ -0,0 +1,1589 @@
+/*
+ * Broadcom SSB DMA controller interface library for Linux
+ *
+ * Copyright (C) 2011, George Kashperko <george@znau.edu.ua>
+ * Copyright (C) 2009, Broadcom Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+/*
+ * v0.98, 2011-02-04 20:55T+2
+ *  initial release (dma 32 untested)
+ * v0.99, 2011-05-07 03:08T+2
+ *  fix linux-2.6.39 build errors and warnings
+ */
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/circ_buf.h>
+#include <linux/ssb/ssb.h>
+#include "linux/ssb/ssb_dma.h"
+
+#define PFX "ssb-dma: "
+
+/** helper defs for circular buffers **/
+/* get slot index */
+#define CIRC_SLOT(slot,size) ((slot) & ((size) - 1))
+/* get next slot index */
+#define CIRC_NEXT(slot,size) CIRC_SLOT((slot) + 1, (size))
+
+/* dma channel ops */
+#define __define_op(b,d,o) .o = ssb_dma##b##_##d##o
+struct ssb_dma_ops {
+	enum ssb_dma_result (*state)(struct ssb_dma *dma,
+				     enum ssb_dma_state *state,
+				     enum ssb_dma_error *error);
+	enum ssb_dma_result (*reset)(struct ssb_dma *dma);
+	enum ssb_dma_result (*queue)(struct ssb_dma *dma);
+	enum ssb_dma_result (*drinit)(struct ssb_dma *dma);
+	enum ssb_dma_result (*setupdd)(struct ssb_dma *dma, u16 index,
+				       dma_addr_t mapping, u16 length,
+				       u32 flags);
+};
+
+/* ssb dma physical address */
+union ssb_dma_addr {
+	ssb_dma_addr_t pa;
+	struct {
+		u32 lo;
+#ifdef CONFIG_SSB_DMA64
+		u32 hi;
+#endif
+	};
+};
+
+/* private struct used at channels' initialization time */
+struct ssb_dma_initdata {
+	u32 caps; /* common channel capabilities */
+
+	gfp_t gfp; /* dma memory allocations' kernel flags */
+
+	u8 nr_dma; /* number of channels */
+
+	u16 max_slots; /* max number of transmission slots */
+
+	u8 drboundary; /* descriptor ring boundary, bits */
+	u8 ddsize; /* single dma descriptor size */
+
+	union ssb_dma_addr drpb; /* descriptor ring base translation */
+	union ssb_dma_addr datapb; /* data base translation */
+};
+
+/* dma packet descriptor */
+struct ssb_dma_packet {
+	void *buffer; /* untyped driver-specific data buffer pointer */
+	dma_addr_t datap; /* packet data physical address mapping */
+	u16 length; /* buffer data length */
+};
+
+/* data struct describing coherent allocation being fixed
+ * by ssb_dma_alloc_coherent to meet ssb dma controller
+ * alignment constraints */
+struct ssb_dma_coherent_alloc_info {
+	/* virtual address allocated by dma_alloc_coherent,
+	 * and modified by @offset to make it fit controller's
+	 * alignment requirments. used as search key value. */
+	void *va;
+	/* offset added to both virtual and physical addresses
+	 * to meet alignment constraints set by ssb dma controller */
+	u32 offset;
+	u16 size; /* allocation size */
+
+	struct list_head list; /* allocations' list */
+};
+
+/* dma mapping validation */
+/* TODO: maybe add dma mask validation ? */
+static inline void
+SSB_DMA_CHKMAPPING(struct ssb_dma *dma, ssb_dma_addr_t pb, dma_addr_t pa)
+{
+#if defined(CONFIG_SSB_DMA_DEBUG) && defined(CONFIG_PHYS_ADDR_T_64BIT)
+	bool dma64 = dma->cfg->caps & SSB_DMA_64BIT;
+# ifdef CONFIG_SSB_DMA64
+	SSB_DMA_BUG_ON(dma64 && (u32)pb && (pa >> 32));
+# endif /* CONFIG_SSB_DMA64 */
+# ifdef CONFIG_SSB_DMA32
+	SSB_DMA_BUG_ON(!dma64 && ((pb | pa) >> 32));
+# endif /* CONFIG_SSB_DMA32 */
+#endif /* CONFIG_SSB_DMA_DEBUG && CONFIG_PHYS_ADDR_T_64BIT */
+}
+
+/** inline device driver ops helpers **/
+static inline void *
+ssb_dma_driver_buf_alloc(struct ssb_dma *dma, u16 length, void **data)
+{
+	SSB_DMA_BUG_ON(!dma->driver_ops);
+	SSB_DMA_BUG_ON(!dma->driver_ops->buf_alloc);
+
+	return dma->driver_ops->buf_alloc(dma, length, data);
+}
+
+static inline void
+ssb_dma_driver_buf_free(struct ssb_dma *dma, void *buffer)
+{
+	SSB_DMA_BUG_ON(!dma->driver_ops);
+	SSB_DMA_BUG_ON(!dma->driver_ops->buf_free);
+
+	dma->driver_ops->buf_free(dma, buffer);
+}
+
+/** inline channel ops helpers **/
+/* query channel state */
+enum ssb_dma_result
+ssb_dma_state(struct ssb_dma *dma, enum ssb_dma_state *state,
+	      enum ssb_dma_error *error)
+{
+	return dma->ops->state(dma, state, error);
+}
+EXPORT_SYMBOL(ssb_dma_state);
+
+/* process pending queue */
+enum ssb_dma_result
+ssb_dma_queue(struct ssb_dma *dma)
+{
+	return dma->ops->queue(dma);
+}
+EXPORT_SYMBOL(ssb_dma_queue);
+
+static inline enum ssb_dma_result
+ssb_dma_drinit(struct ssb_dma *dma)
+{
+	return dma->ops->drinit(dma);
+}
+
+/*
+ * @dma:     dma channel
+ * @index:   descriptor index in channel descriptor ring table
+ * @mapping: physical address valid for dma controller
+ *	     transfers
+ * @length:  length of data referenced by descriptor
+ * @flags:   descriptor flags as of ssb_dma_dd32.control
+ *	     and ssb_dma_dd64.control1 layout */
+static inline void
+ssb_dma_setupdd(struct ssb_dma *dma, u16 index, dma_addr_t mapping,
+		u16 length, u32 flags)
+{
+	dma->ops->setupdd(dma, index, mapping, length, flags);
+}
+
+/** inline controller ops helpers **/
+
+/** io access helpers **/
+static inline u32
+ssb_dma_read32(struct ssb_dma *dma, u32 offset)
+{
+	return ssb_read32(dma->cfg->dev, dma->info.regs + offset);
+}
+
+static inline void
+ssb_dma_write32(struct ssb_dma *dma, u32 offset, u32 value)
+{
+	ssb_write32(dma->cfg->dev, dma->info.regs + offset, value);
+}
+
+
+static inline void
+ssb_dma_set32(struct ssb_dma *dma, u32 offset, u32 value)
+{
+	struct ssb_device *dev = dma->cfg->dev;
+	offset += dma->info.regs;
+	ssb_write32(dev, offset, ssb_read32(dev, offset) | value);
+}
+
+static inline void
+ssb_dma_maskset32(struct ssb_dma *dma, u32 offset, u32 mask, u32 value)
+{
+	struct ssb_device *dev = dma->cfg->dev;
+	offset += dma->info.regs;
+	ssb_write32(dev, offset, (ssb_read32(dev, offset) & mask) | value);
+}
+
+/* wait for specific masked bits state.
+ * @timeout:        states maximum wait time in tens of useconds
+ * @expected_state: true for truth value comparison expected, false otherwise
+ *
+ * returns masked register value regardless of wait success/failure
+ */
+static u32
+ssb_dma_waitmask32(struct ssb_dma *dma, u32 offset, u32 mask, u32 bits,
+		   u32 timeout, bool expected_state)
+{
+	u32 i, tmp = 0;
+
+	SSB_DMA_BUG_ON(!timeout);
+
+	for (i = 0; i < timeout; i++) {
+		tmp = ssb_dma_read32(dma, offset) & mask;
+		if (expected_state && (tmp == bits))
+			return tmp;
+		if (!expected_state && !(tmp == bits))
+			return tmp;
+		udelay(10);
+	}
+	SSB_DMA_WARN_ON(1);
+	return tmp;
+}
+
+#ifdef CONFIG_SSB_DMA_DEBUG
+void
+ssb_dma_dumpinfo(struct ssb_dma *dma, bool verbose)
+{
+	bool dma64 = dma->cfg->caps & SSB_DMA_64BIT;
+
+	SSB_DMA_TRACE(
+		"SSB DMA CHANNEL[ %d ]\n"
+		"info {\n"
+		" .name: %s\n"
+		" .regs: 0x%04x\n"
+		" .nr_slots: %d\n"
+		" .dir: %s\n"
+		"}\n"
+		"drboundary: %d\n"
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+		"drp: 0x%016llx\n"
+#else
+		"drp: 0x%08x\n"
+#endif
+		"drv: 0x%p\n"
+		"drsize: %d\n"
+		"slot: %d\n"
+		"nr_posted: %d\n",
+		dma->idx,
+		dma->info.name,
+		dma->info.regs,
+		dma->info.nr_slots,
+		(dma->info.dir == DMA_TO_DEVICE) ? "TX" : (
+			(dma->info.dir == DMA_FROM_DEVICE) ? "RX" : "UNKNOWN"),
+		dma->drboundary,
+		dma->drp,
+		dma->drv,
+		dma->drsize,
+		dma->slot,
+		dma->nr_posted);
+
+	if (verbose) {
+		if (dma64) {
+#ifdef CONFIG_SSB_DMA64
+			SSB_DMA_TRACE(
+				"\n"
+				"control: 0x%08x\n"
+				"ptr: 0x%08x\n"
+				"address_lo: 0x%08x\n"
+				"address_hi: 0x%08x\n"
+				"status0: 0x%08x\n"
+				"status1: 0x%08x\n",
+				ssb_dma_read32(dma, SSB_DMA_CONTROL),
+				ssb_dma_read32(dma, SSB_DMA_64PTR),
+				ssb_dma_read32(dma, SSB_DMA_64ADDRLO),
+				ssb_dma_read32(dma, SSB_DMA_64ADDRHI),
+				ssb_dma_read32(dma, SSB_DMA_64STATUS0),
+				ssb_dma_read32(dma, SSB_DMA_64STATUS1));
+#endif /* CONFIG_SSB_DMA64 */
+		} else {
+#ifdef CONFIG_SSB_DMA32
+			SSB_DMA_TRACE(
+				"\n"
+				"control: 0x%08x\n"
+				"address: 0x%08x\n"
+				"ptr: 0x%08x\n"
+				"status: 0x%08x\n",
+				ssb_dma_read32(dma, SSB_DMA_CONTROL),
+				ssb_dma_read32(dma, SSB_DMA_32ADDR),
+				ssb_dma_read32(dma, SSB_DMA_32PTR),
+				ssb_dma_read32(dma, SSB_DMA_32STATUS));
+#endif /* CONFIG_SSB_DMA32 */
+		}
+	}
+}
+EXPORT_SYMBOL(ssb_dma_dumpinfo);
+
+void
+ssb_dma_config_dumpinfo(struct ssb_dma_config *cfg)
+{
+	SSB_DMA_TRACE(
+		"SSB DMA CONTROLLER\n"
+		"device id: 0x%04x\n"
+		"caps: (0x%02x) {\n"
+		" .dma addr width: %d\n"
+		" .dma64: %s\n"
+		" .addrext suport: %s\n"
+		" .parity: %s\n"
+		" .64bit16balign: %s\n"
+		"}\n"
+		"coherent_gfp: 0x%x\n"
+		"ddsize: %d\n"
+#ifdef CONFIG_SSB_DMA64
+		"drpb: 0x%016llx\n"
+		"datapb: 0x%016llx\n"
+#else
+		"drpb: 0x%08x\n"
+		"datapb: 0x%08x\n"
+#endif
+		"nr_dma: %d\n",
+		cfg->dev->id.coreid,
+		cfg->caps,
+		cfg->caps & SSB_DMA_ADDRMASK,
+		(cfg->caps & SSB_DMA_64BIT) ? "yes" : "no",
+		(cfg->caps & SSB_DMA_AE) ? "yes" : "no",
+		(cfg->caps & SSB_DMA_PARITY) ? "yes" : "no",
+		(cfg->caps & SSB_DMA_64BIT16BALIGN) ? "yes" : "no",
+		cfg->coherent_gfp,
+		cfg->ddsize,
+		cfg->drpb,
+		cfg->datapb,
+		cfg->nr_dma);
+}
+EXPORT_SYMBOL(ssb_dma_config_dumpinfo);
+#endif /* CONFIG_SSB_DMA_DEBUG */
+
+/** misc. inline helpers **/
+
+/** 32-bit cores specific routines **/
+#ifdef CONFIG_SSB_DMA32
+/* 32-bit ssb dma address translation
+ * @pa: physical address being translated
+ * @pb: physical address translation base
+ * returns Address Extension bits as of SSB_DMA_CONTROL layout
+ */
+static inline u32
+ssb_dma32_translation(dma_addr_t mapping, ssb_dma_addr_t pb,
+		      union ssb_dma_addr *pa)
+{
+	u32 ae;
+	pa->lo = mapping;
+	if ((u32)pb) {
+		if ((ae = pa->lo & SSB_DMA_TRANSLATION_MASK)) {
+			/* shift the high bit(s) from pa to ae */
+			ae >>= SSB_DMA_TRANSLATION_SHIFT;
+			ae <<= SSB_DMA_CONTROL_AE_SHIFT;
+			pa->lo &= ~SSB_DMA_TRANSLATION_MASK;
+		}
+	} else {
+		ae = 0;
+	}
+	pa->lo += (u32)pb;
+	return ae;
+}
+
+/* query channel state */
+static enum ssb_dma_result
+ssb_dma32_state(struct ssb_dma *dma, enum ssb_dma_state *state,
+		enum ssb_dma_error *error)
+{
+	u32 tmp = ssb_dma_read32(dma, SSB_DMA_32STATUS);
+	if (state)
+		*state = SSB_DMA_32STATUS_STATE(tmp);
+	if (error)
+		*error = SSB_DMA_32STATUS_ERR(tmp);
+
+	return (tmp == 0xFFFFFFFF) ? -ESSB_DMA_COREFAIL : ESSB_DMA_OK;
+}
+
+/* initialize descriptor ring table */
+static enum ssb_dma_result
+ssb_dma32_drinit(struct ssb_dma *dma)
+{
+	union ssb_dma_addr drp;
+	u32 ae = ssb_dma32_translation(dma->cfg->drpb, dma->drp, &drp);
+
+	if (ae)
+		ssb_dma_maskset32(dma, SSB_DMA_CONTROL,
+				  ~SSB_DMA_CONTROL_AE_MASK, ae);
+
+	ssb_dma_write32(dma, SSB_DMA_32ADDR, drp.lo);
+
+	return ESSB_DMA_OK;
+}
+
+/* init descriptor */
+static enum ssb_dma_result
+ssb_dma32_setupdd(struct ssb_dma *dma, u16 index, dma_addr_t mapping,
+		  u16 length, u32 flags)
+{
+	struct ssb_dma_dd32 *dd32 = &dma->drv32[index];
+	union ssb_dma_addr datap;
+	u32 ae = ssb_dma32_translation(dma->cfg->datapb, mapping, &datap);
+
+	SSB_DMA_BUG_ON(index >= dma->info.nr_slots);
+
+	if (index == dma->info.nr_slots - 1)
+		/* end of table flag on last entry */
+		flags |= SSB_DMA_DDCTRL_EOT;
+
+	/* TODO: calc parity. */
+
+	dd32->control = cpu_to_le32(length | ae | flags);
+	dd32->address = cpu_to_le32(datap.lo);
+
+	return ESSB_DMA_OK;
+}
+#endif /* CONFIG_SSB_DMA32 */
+
+/** 64-bit cores specific routines **/
+#ifdef CONFIG_SSB_DMA64
+static inline u32
+__addr_hi(ssb_dma_addr_t a)
+{
+	return a >> 32;
+}
+
+/* 64-bit ssb dma address translation
+ * @pa: physical address being translated
+ * @pb: physical address translation base
+ * returns Address Extension bits as of SSB_DMA_CONTROL layout
+ */
+static inline u32
+ssb_dma64_translation(dma_addr_t mapping, ssb_dma_addr_t pb,
+		      union ssb_dma_addr *pa)
+{
+	u32 ae;
+	pa->pa = mapping;
+	if ((u32)pb) {
+		if ((ae = pa->lo & SSB_DMA_TRANSLATION_MASK)) {
+			/* shift the high bit(s) from pa to ae */
+			ae >>= SSB_DMA_TRANSLATION_SHIFT;
+			ae <<= SSB_DMA_CONTROL_AE_SHIFT;
+			pa->lo &= ~SSB_DMA_TRANSLATION_MASK;
+			pa->hi = __addr_hi(pb);
+		} else {
+			goto pa_hi;
+		}
+	} else {
+		ae = 0;
+pa_hi:
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+		pa->hi += __addr_hi(pb);
+#else
+		pa->hi = __addr_hi(pb);
+#endif
+	}
+	pa->lo += (u32)pb;
+	return ae;
+}
+
+/* query channel state */
+static enum ssb_dma_result
+ssb_dma64_state(struct ssb_dma *dma, enum ssb_dma_state *state,
+		enum ssb_dma_error *error)
+{
+	u32 tmp0, tmp1;
+	tmp0 = ssb_dma_read32(dma, SSB_DMA_64STATUS0);
+	tmp1 = ssb_dma_read32(dma, SSB_DMA_64STATUS1);
+	if (state)
+		*state = SSB_DMA_64STATUS0_STATE(tmp0);
+	if (error)
+		*error = SSB_DMA_64STATUS1_ERR(tmp1);
+
+	return ((tmp0 == 0xFFFFFFFF) || (tmp1 == 0xFFFFFFFF)) ?
+		-ESSB_DMA_COREFAIL : ESSB_DMA_OK;
+}
+
+/* initialize descriptor ring table */
+static enum ssb_dma_result
+ssb_dma64_drinit(struct ssb_dma *dma)
+{
+	union ssb_dma_addr drp;
+	u32 ae = ssb_dma64_translation(dma->cfg->drpb, dma->drp, &drp);
+
+	if (ae)
+		ssb_dma_maskset32(dma, SSB_DMA_CONTROL,
+				  ~SSB_DMA_CONTROL_AE_MASK, ae);
+
+	ssb_dma_write32(dma, SSB_DMA_64ADDRLO, drp.lo);
+	ssb_dma_write32(dma, SSB_DMA_64ADDRHI, drp.hi);
+
+	return ESSB_DMA_OK;
+}
+
+/* init descriptor */
+static enum ssb_dma_result
+ssb_dma64_setupdd(struct ssb_dma *dma, u16 index, dma_addr_t mapping,
+		  u16 length, u32 flags)
+{
+	struct ssb_dma_dd64 *dd64 = &dma->drv64[index];
+	union ssb_dma_addr datap;
+	u32 ae = ssb_dma64_translation(dma->cfg->datapb, mapping, &datap);
+
+	SSB_DMA_BUG_ON(index >= dma->info.nr_slots);
+
+	if (index == dma->info.nr_slots - 1)
+	        /* end of table flag on last entry */
+		flags |= SSB_DMA_DDCTRL_EOT;
+
+	/* TODO: calc parity. */
+
+	dd64->control0 = cpu_to_le32(flags);
+	dd64->control1 = cpu_to_le32(ae | length);
+	dd64->address_lo = cpu_to_le32(datap.lo);
+	dd64->address_hi = cpu_to_le32(datap.hi);
+
+	return ESSB_DMA_OK;
+}
+#endif /* CONFIG_SSB_DMA64 */
+
+/** allocation helpers **/
+
+/* ssb_dma_alloc_coherent() - try to allocate coherent dma memory block,
+ *                            meeting ssb dma controller alignment constraints
+ *
+ * @dma:       ssb dma controller for which memory is being allocated
+ * @size:      size of requested memory area
+ * @dma_hande: will be filled with physical address valid for
+ *             dma controller operations
+ *
+ * on success returns virtual address, which meets alignment constraints
+ * set by given ssb dma controller
+ *
+ * memory allocated must be released by ssb_dma_free_coherent call
+ */
+static void *
+ssb_dma_alloc_coherent(struct ssb_dma *dma, u16 size, dma_addr_t *dma_handle)
+{
+	void *va_orig, *va;
+	dma_addr_t pa_orig, pa;
+	u16 offset;
+	u16 alignbytes = 1 << dma->drboundary;
+	u16 alignmask = alignbytes - 1;
+
+	va_orig = dma_alloc_coherent(dma->cfg->dev->dma_dev, size,
+				     &pa_orig, dma->cfg->coherent_gfp);
+	if (!va_orig)
+		return NULL;
+
+	/* ensure pointer is properly aligned and data fits block requested */
+	pa = (pa_orig + alignmask) & ~alignmask;
+	offset = pa - pa_orig;
+	if (offset) {
+		struct ssb_dma_coherent_alloc_info *ca;
+		ulong extrasize = alignbytes - PAGE_SIZE;
+
+		dma_free_coherent(dma->cfg->dev->dma_dev, size, va_orig, pa_orig);
+		if (!(ca = kmalloc(sizeof(*ca), GFP_KERNEL)))
+			return NULL;
+
+		/* realloc larger block accounting worse possible scenario */
+		size += extrasize;
+		va_orig = dma_alloc_coherent(dma->cfg->dev->dma_dev, size,
+					     &pa_orig, dma->cfg->coherent_gfp);
+		if (!va_orig) {
+			kfree(ca);
+			return NULL;
+		}
+		pa = (pa_orig + alignmask) & ~alignmask;
+		offset = pa - pa_orig;
+		va = va_orig + offset;
+
+		/* Make sure data block requested fits block allocated */
+		SSB_DMA_BUG_ON(offset > extrasize);
+
+		/* save allocations' data for subsequent release
+		 * by ssb_dma_free_coherent */
+		ca->va = va;
+		ca->offset = offset;
+		ca->size = size;
+		list_add(&ca->list, &dma->cfg->ca_list);
+	} else {
+		va = va_orig;
+	}
+
+	/* Make sure addresses didn't overflow */
+	SSB_DMA_BUG_ON((pa_orig > pa) || (va_orig > va));
+	/* Make sure alignment is correct */
+	SSB_DMA_BUG_ON((u32)pa & (alignbytes - 1));
+
+	*dma_handle = pa;
+	return va;
+}
+
+/* ssb_dma_free_coherent() - free coherent memory block, previously allocated
+ *                           by ssb_dma_alloc_coherent()
+ */
+static void
+ssb_dma_free_coherent(struct ssb_dma *dma, u16 size, void *vaddr,
+		      dma_addr_t dma_handle)
+{
+	struct ssb_dma_coherent_alloc_info *ca, *tmp;
+
+	/* check if block being released was workarounded to fit
+	 * dma controller's alignment requirments */
+	list_for_each_entry_safe(ca, tmp, &dma->cfg->ca_list, list) {
+		if (ca->va == vaddr) {
+			vaddr -= ca->offset;
+			dma_handle -= ca->offset;
+			size = ca->size;
+			list_del(&ca->list);
+			kfree(ca);
+			break;
+		}
+	}
+
+	dma_free_coherent(dma->cfg->dev->dma_dev, size, vaddr, dma_handle);
+}
+
+/*** channel management routines ***/
+/** generic channel routines **/
+/* alter channel control flags */
+void
+ssb_dma_cflags(struct ssb_dma *dma, u32 mask, u32 flags)
+{
+	SSB_DMA_WARN_ON(flags & ((dma->info.dir == DMA_TO_DEVICE) ?
+		    SSB_DMA_CTL_RXMASK : SSB_DMA_CTL_TXMASK));
+
+	if ((flags & mask) == SSB_DMA_CTL_PE &&
+	    !(dma->cfg->caps & SSB_DMA_PARITY))
+		flags &= ~SSB_DMA_CTL_PE;
+
+	/* TODO: disable parity checks by now */
+	flags &= ~SSB_DMA_CTL_PE;
+
+	dma->cflags = (dma->cflags & ~mask) | flags;
+}
+EXPORT_SYMBOL(ssb_dma_cflags);
+
+/* check if channel is enabled */
+inline bool
+ssb_dma_enabled(struct ssb_dma *dma)
+{
+	u32 tmp = ssb_dma_read32(dma, SSB_DMA_CONTROL);
+	return ((tmp != 0xFFFFFFFF) && (tmp & SSB_DMA_CONTROL_OPEN));
+}
+
+/* stop channel */
+enum ssb_dma_result
+ssb_dma_disable(struct ssb_dma *dma)
+{
+	u32 tmp;
+	bool istx = dma->info.dir == DMA_TO_DEVICE;
+	u32 reg, state, mask;
+
+	if (dma->cfg->caps & SSB_DMA_64BIT) {
+		reg = SSB_DMA_64STATUS0;
+		state = SSB_DMA_64STATUS0_STATE_SUSP;
+		mask = SSB_DMA_64STATUS0_STATE_MASK;
+	} else {
+		reg = SSB_DMA_32STATUS;
+		state = SSB_DMA_32STATUS_STATE_SUSP;
+		mask = SSB_DMA_32STATUS_STATE_MASK;
+	}
+
+	/* for tx channels suspend dma first */
+	if (istx) {
+		ssb_dma_write32(dma, SSB_DMA_CONTROL, SSB_DMA_CONTROL_TXSE);
+		tmp = ssb_dma_waitmask32(dma, reg, state, state, 1000, false);
+#ifdef CONFIG_SSB_DMA_DEBUG
+		/* TODO: remove after some testing */
+		if (dma->cfg->caps & SSB_DMA_64BIT) {
+			SSB_DMA_WARN_ON(ssb_dma_read32(dma, SSB_DMA_64STATUS0) &
+					(SSB_DMA_64STATUS0_STATE_ACTIVE |
+					 (SSB_DMA_64STATUS0_STATE_MASK &
+					 ~(SSB_DMA_64STATUS0_STATE_IDLE |
+					   SSB_DMA_64STATUS0_STATE_STOPPED |
+					   SSB_DMA_64STATUS0_STATE_SUSP))));
+		} else {
+			SSB_DMA_WARN_ON(ssb_dma_read32(dma, SSB_DMA_32STATUS) &
+					(SSB_DMA_32STATUS_STATE_ACTIVE |
+					 (SSB_DMA_32STATUS_STATE_MASK &
+					 ~(SSB_DMA_32STATUS_STATE_IDLE |
+					   SSB_DMA_32STATUS_STATE_STOPPED |
+					   SSB_DMA_32STATUS_STATE_SUSP))));
+		}
+#endif
+	}
+
+	ssb_dma_write32(dma, SSB_DMA_CONTROL, 0);
+	tmp = ssb_dma_waitmask32(dma, state, mask, 0, 1000, true);
+
+	/* wait for the last transaction to complete */
+	udelay(300);
+
+	return (tmp == 0) ? ESSB_DMA_OK : -ESSB_DMA_TIMEDOUT;
+}
+EXPORT_SYMBOL(ssb_dma_disable);
+
+/* set descriptor pointer register */
+static void inline
+ssb_dma_ptr(struct ssb_dma *dma, u16 slot)
+{
+	SSB_DMA_BUG_ON(slot >= dma->info.nr_slots);
+
+	if (dma->cfg->caps & SSB_DMA_64BIT) {
+#ifdef CONFIG_SSB_DMA64
+		/* slot * sizeof(struct ssb_dma_dd64) */
+		ssb_dma_write32(dma, SSB_DMA_64PTR, slot << 4);
+#endif
+	} else {
+#ifdef CONFIG_SSB_DMA32
+		/* slot * sizeof(struct ssb_dma_dd32) */
+		ssb_dma_write32(dma, SSB_DMA_32PTR, slot << 3);
+#endif
+	}
+}
+
+/* get descriptor index pointed by channel's cdp pointer */
+static u32 inline
+ssb_dma_cdi(struct ssb_dma *dma)
+{
+	if (dma->cfg->caps & SSB_DMA_64BIT) {
+#ifdef CONFIG_SSB_DMA64
+		/* cdp / sizeof(struct ssb_dma_dd64) */
+		return SSB_DMA_64STATUS0_CDP(
+			ssb_dma_read32(dma, SSB_DMA_64STATUS0)) >> 4;
+#endif
+	} else {
+#ifdef CONFIG_SSB_DMA32
+		/* cdp / sizeof(struct ssb_dma_dd32) */
+		return SSB_DMA_32STATUS_CDP(
+			ssb_dma_read32(dma, SSB_DMA_32STATUS)) >> 3;
+#endif
+	}
+	SSB_DMA_BUG_ON(1);
+	return -ESSB_DMA_UNSPEC;
+}
+
+/* get descriptor index pointed by channel's adp pointer */
+static u32 inline
+ssb_dma_adi(struct ssb_dma *dma)
+{
+	if (dma->cfg->caps & SSB_DMA_64BIT) {
+#ifdef CONFIG_SSB_DMA64
+		/* adp / sizeof(struct ssb_dma_dd64) */
+		return SSB_DMA_64STATUS1_ADP(
+			ssb_dma_read32(dma, SSB_DMA_64STATUS1)) >> 4;
+#endif
+	} else {
+#ifdef CONFIG_SSB_DMA32
+		/* cdp / sizeof(struct ssb_dma_dd32) */
+		return SSB_DMA_32STATUS_ADP(
+			ssb_dma_read32(dma, SSB_DMA_32STATUS)) >> 3;
+#endif
+	}
+	SSB_DMA_BUG_ON(1);
+	return -ESSB_DMA_UNSPEC;
+}
+
+/* enable channel operations */
+enum ssb_dma_result
+ssb_dma_enable(struct ssb_dma *dma)
+{
+	/* 64-bit dma cores with 16-byte descriptor alignment
+	 * require descriptor ring pointer to be initialized before
+	 * enabling the channel */
+	bool earlydd64init = dma->cflags & SSB_DMA_64BIT16BALIGN;
+	bool isrx = dma->info.dir == DMA_FROM_DEVICE;
+	enum ssb_dma_result res;
+	u32 tmp = SSB_DMA_CONTROL_OPEN;
+
+	if (!(dma->cflags & SSB_DMA_CTL_PE))
+		tmp |= SSB_DMA_CONTROL_PD;
+
+	if (isrx) {
+		if (dma->cflags & SSB_DMA_CTL_RXOC)
+			tmp |= SSB_DMA_CONTROL_RXOC;
+		tmp |= dma->rx.hdrsize << SSB_DMA_CONTROL_RXFO_SHIFT;
+	}
+
+	if (earlydd64init)
+		if ((res = ssb_dma_drinit(dma)))
+			return res;
+
+	/* enable engine */
+	ssb_dma_write32(dma, SSB_DMA_CONTROL, tmp);
+
+	if (!earlydd64init)
+		if ((res = ssb_dma_drinit(dma)))
+			return res;
+
+	if (isrx)
+		/* This will trigger rx controller from
+		 * IDLE state to ACTIVE */
+		ssb_dma_ptr(dma, dma->info.nr_slots - 1);
+
+	return ssb_dma_enabled(dma) ? ESSB_DMA_OK : -ESSB_DMA_COREFAIL;
+}
+EXPORT_SYMBOL(ssb_dma_enable);
+
+/* free packet pointer vector */
+static void
+ssb_dma_freepackets(struct ssb_dma *dma)
+{
+	struct ssb_dma_packet *packet;
+	int i;
+
+	SSB_DMA_BUG_ON(!dma->packets);
+
+	for (i = 0, packet = dma->packets; i < dma->info.nr_slots;
+	     i++, packet++) {
+		if (!packet->buffer)
+			continue;
+		SSB_DMA_BUG_ON(!packet->datap);
+		SSB_DMA_BUG_ON(!packet->length);
+
+		dma_unmap_single(dma->cfg->dev->dma_dev, packet->datap,
+			packet->length, dma->info.dir);
+		ssb_dma_driver_buf_free(dma, packet->buffer);
+	}
+	memset(dma->packets, 0, dma->info.nr_slots *
+				sizeof(struct ssb_dma_packet));
+}
+
+/** tx channel routines **/
+/* check if channel is suspended (tx only) */
+bool
+ssb_dma_suspended(struct ssb_dma *dma)
+{
+	u32 tmp = ssb_dma_read32(dma, SSB_DMA_CONTROL) & SSB_DMA_CONTROL_TXSE;
+	return tmp == SSB_DMA_CONTROL_TXSE;
+}
+
+/* suspend channel (tx only) */
+void
+ssb_dma_suspend(struct ssb_dma *dma)
+{
+	ssb_dma_set32(dma, SSB_DMA_CONTROL, SSB_DMA_CONTROL_TXSE);
+}
+
+/* resume channel (tx only) */
+void
+ssb_dma_resume(struct ssb_dma *dma)
+{
+	ssb_dma_maskset32(dma, SSB_DMA_CONTROL, ~SSB_DMA_CONTROL_TXSE, 0);
+}
+
+/* prepare tx channel */
+enum ssb_dma_result
+ssb_dma_txinit(struct ssb_dma *dma, struct ssb_dma_driver_ops *driver_ops)
+{
+	SSB_DMA_BUG_ON(dma->info.dir != DMA_TO_DEVICE);
+	SSB_DMA_BUG_ON(!dma->drv);
+	SSB_DMA_BUG_ON(!dma->packets);
+
+	dma->driver_ops = driver_ops;
+
+	return ESSB_DMA_OK;
+}
+EXPORT_SYMBOL(ssb_dma_txinit);
+
+/* process pending tx queue */
+enum ssb_dma_result
+ssb_dma_txqueue(struct ssb_dma *dma)
+{
+	struct ssb_dma_packet *packet;
+	u16 nr_sent;
+	u16 cdi;
+
+	SSB_DMA_BUG_ON(dma->info.dir != DMA_TO_DEVICE);
+
+	if (!dma->nr_posted)
+		return ESSB_DMA_OK;
+
+	/* check if there were any packets transmitted */
+	cdi = ssb_dma_cdi(dma);
+	if (dma->slot == cdi)
+		return ESSB_DMA_OK;
+
+	nr_sent = CIRC_CNT(cdi, dma->slot, dma->info.nr_slots);
+	packet = &dma->packets[dma->slot];
+	while (true) {
+		SSB_DMA_BUG_ON(!(packet->buffer && packet->datap &&
+				 packet->length));
+
+		dma_unmap_single(dma->cfg->dev->dma_dev, packet->datap,
+				 packet->length, DMA_TO_DEVICE);
+		ssb_dma_driver_buf_free(dma, packet->buffer);
+
+		memset(packet, 0, sizeof(struct ssb_dma_packet));
+		ssb_dma_setupdd(dma, dma->slot, 0, 0, 0);
+
+		dma->nr_posted--;
+		dma->slot = CIRC_NEXT(dma->slot, dma->info.nr_slots);
+		if (!--nr_sent)
+			break;
+		if (dma->slot)
+			packet++;
+		else
+			packet = dma->packets;
+	};
+
+	return ESSB_DMA_OK;
+}
+
+/* post buffer to transfer queue */
+enum ssb_dma_result
+ssb_dma_txsingle(struct ssb_dma *dma, void *buffer, void *data, u16 length,
+		 bool commit)
+{
+	struct ssb_dma_packet *packet;
+	union ssb_dma_addr datap;
+	u32 slot;
+
+	SSB_DMA_BUG_ON(dma->info.dir != DMA_TO_DEVICE);
+
+	/* check if there is place for new packet */
+	if (dma->nr_posted == dma->info.nr_slots - 1)
+		return -ESSB_DMA_NOFREESLOTS;
+
+	/* map data for dma transfer */
+	datap.pa = dma_map_single(dma->cfg->dev->dma_dev, data, length,
+				  DMA_TO_DEVICE);
+	if (dma_mapping_error(dma->cfg->dev->dma_dev, datap.pa))
+		return -ESSB_DMA_DATAMAP;
+
+	SSB_DMA_CHKMAPPING(dma, dma->cfg->datapb, datap.pa);
+
+	slot = CIRC_SLOT(dma->slot + dma->nr_posted, dma->info.nr_slots);
+	ssb_dma_setupdd(dma, slot, datap.pa, length, SSB_DMA_DDCTRL_IOC |
+						     SSB_DMA_DDCTRL_SOF |
+						     SSB_DMA_DDCTRL_EOF);
+	/* save packet data */
+	packet = &dma->packets[slot];
+	packet->buffer = buffer;
+	packet->datap = datap.pa;
+	packet->length = length;
+
+	dma->nr_posted++;
+
+	if (commit) {
+		/* point next descriptor after current */
+		slot = CIRC_NEXT(slot, dma->info.nr_slots);
+#ifndef CONFIG_BCM47XX
+		/* make sure changes to descriptor ring table are commited
+		 * before we let controller access them */
+		wmb();
+#endif
+		/* post descriptor to chip */
+		ssb_dma_ptr(dma, slot);
+
+	    /*	TODO: remove
+		printk("t%d", dma->idx);
+	    */
+	}
+
+	return ESSB_DMA_OK;
+}
+EXPORT_SYMBOL(ssb_dma_txsingle);
+
+/** rx channel routines **/
+/* allocate buffers for rx queue */
+enum ssb_dma_result
+ssb_dma_rxfill(struct ssb_dma *dma)
+{
+	struct ssb_dma_packet *packet;
+	enum ssb_dma_result res;
+	u16 nr_topost = dma->info.nr_slots - dma->nr_posted;
+	union ssb_dma_addr datap;
+	u16 slot;
+	void *buffer;
+	void *datav;
+
+	SSB_DMA_BUG_ON(dma->info.dir != DMA_FROM_DEVICE);
+
+	if (!nr_topost)
+		return ESSB_DMA_OK;
+
+	/* get first free descriptor index */
+	slot = CIRC_SLOT(dma->slot + dma->nr_posted, dma->info.nr_slots);
+	packet = &dma->packets[slot];
+	while (true) {
+		SSB_DMA_BUG_ON(packet->buffer || packet->datap);
+
+		buffer = ssb_dma_driver_buf_alloc(dma, dma->rx.framesize,
+						  &datav);
+		if (!buffer)
+			return -ESSB_DMA_NOMEM;
+		SSB_DMA_BUG_ON(!datav);
+
+		datap.pa = dma_map_single(dma->cfg->dev->dma_dev, datav,
+					  dma->rx.framesize, DMA_FROM_DEVICE);
+		if (dma_mapping_error(dma->cfg->dev->dma_dev, datap.pa)) {
+			res = -ESSB_DMA_DATAMAP;
+			goto err_free;
+		}
+
+		SSB_DMA_CHKMAPPING(dma, dma->cfg->datapb, datap.pa);
+
+		/* must be aligned */
+	    	SSB_DMA_BUG_ON(datap.pa & 0x03);
+
+	    	packet->buffer = buffer;
+	    	packet->datap = datap.pa;
+	    	packet->length = dma->rx.framesize;
+
+		/* setup descriptor */
+		ssb_dma_setupdd(dma, slot, datap.pa, dma->rx.framesize, 0);
+
+		dma->nr_posted++;
+		if (!--nr_topost)
+			break;
+		if ((slot = CIRC_NEXT(slot, dma->info.nr_slots)))
+			packet++;
+		else
+			packet = dma->packets;
+	}
+
+	return ESSB_DMA_OK;
+
+err_free:
+	SSB_DMA_TRACE("failed\n");
+	/* free packets */
+	ssb_dma_driver_buf_free(dma, buffer);
+	return res;
+}
+
+/* prepare rx channel */
+enum ssb_dma_result
+ssb_dma_rxinit(struct ssb_dma *dma, struct ssb_dma_driver_ops *driver_ops,
+		  u16 bufsize, u16 hdrsize)
+{
+	enum ssb_dma_result res;
+
+	SSB_DMA_BUG_ON(dma->info.dir != DMA_FROM_DEVICE);
+	SSB_DMA_BUG_ON(!driver_ops);
+	SSB_DMA_BUG_ON(!dma->drv);
+	SSB_DMA_BUG_ON(!dma->packets);
+
+	dma->driver_ops = driver_ops;
+	dma->rx.framesize = bufsize;
+	dma->rx.hdrsize = hdrsize;
+
+	res = ssb_dma_rxfill(dma);
+	dma->slot = 0;
+
+	if (res)
+		SSB_DMA_TRACE("failed\n");
+	return res;
+}
+EXPORT_SYMBOL(ssb_dma_rxinit);
+
+/* process pending rx queue */
+enum ssb_dma_result
+ssb_dma_rxqueue(struct ssb_dma *dma)
+{
+	enum ssb_dma_result res;
+	u16 slot, old_posted = dma->nr_posted;
+
+	SSB_DMA_BUG_ON(dma->info.dir != DMA_FROM_DEVICE);
+
+	if ((res = ssb_dma_rxfill(dma)))
+		return res;
+	if (old_posted == dma->nr_posted)
+		return ESSB_DMA_OK;
+
+	/* get first free descriptor index */
+	slot = CIRC_SLOT(dma->slot + dma->nr_posted, dma->info.nr_slots);
+
+#ifndef CONFIG_BCM47XX
+	/* make sure changes to descriptor ring table are commited
+	 * before we let controller access them */
+	wmb();
+#endif
+
+	/* post descriptor to chip */
+	ssb_dma_ptr(dma, slot);
+
+	return ESSB_DMA_OK;
+}
+
+/* get next packet received */
+enum ssb_dma_result
+ssb_dma_rxsingle(struct ssb_dma *dma, void **buffer)
+{
+	struct ssb_dma_packet *packet;
+	enum ssb_dma_result res;
+	u16 cdi;
+
+	SSB_DMA_BUG_ON(dma->info.dir != DMA_FROM_DEVICE);
+	SSB_DMA_BUG_ON(!buffer);
+
+	/* check if there were descriptors posted */
+	if (!dma->nr_posted) {
+		res = -ESSB_DMA_DRINGEMPTY;
+		goto err_nodata;
+	}
+
+	/* get dma current descriptor pointer */
+	cdi = ssb_dma_cdi(dma);
+
+	/* check if there is data for reception */
+	if (dma->slot == cdi) {
+		res = -ESSB_DMA_RXNOPENDING;
+		goto err_nodata;
+	}
+
+	packet = &dma->packets[dma->slot];
+	*buffer = packet->buffer;
+
+	dma_unmap_single(dma->cfg->dev->dma_dev, packet->datap,
+			 dma->rx.framesize, DMA_FROM_DEVICE);
+	memset(packet, 0, sizeof(struct ssb_dma_packet));
+	ssb_dma_setupdd(dma, dma->slot, 0, 0, 0);
+
+	/* adjust counters */
+	dma->slot = CIRC_NEXT(dma->slot, dma->info.nr_slots);
+	dma->nr_posted--;
+
+    /*	TODO: remove
+	printk("r");
+    */
+
+	return ESSB_DMA_OK;
+
+err_nodata:
+	if (res != -ESSB_DMA_RXNOPENDING)
+		printk("R");
+	*buffer = NULL;
+	return res;
+}
+EXPORT_SYMBOL(ssb_dma_rxsingle);
+
+/* reset dma channel state to defaults */
+enum ssb_dma_result
+ssb_dma_reset(struct ssb_dma *dma)
+{
+	enum ssb_dma_result res;
+
+	SSB_DMA_BUG_ON(ssb_dma_enabled(dma));
+
+	if (dma->info.dir == DMA_FROM_DEVICE) {
+		res = ssb_dma_rxfill(dma);
+		if (!res)
+			dma->slot = 0;
+	} else {
+		ssb_dma_freepackets(dma);
+		dma->slot = dma->nr_posted = 0;
+		res = ESSB_DMA_OK;
+	}
+
+	return res;
+}
+EXPORT_SYMBOL(ssb_dma_reset);
+
+/** dma channel ops **/
+
+#ifdef CONFIG_SSB_DMA32
+/* 32-bit tx channel ops */
+static const struct ssb_dma_ops ssb_dma32_tx_ops = {
+	__define_op(32,,    state),
+	__define_op(,   tx, queue),
+	__define_op(32,,    drinit),
+	__define_op(32,,    setupdd),
+};
+
+/* 32-bit rx channels ops */
+static const struct ssb_dma_ops ssb_dma32_rx_ops = {
+	__define_op(32,,    state),
+	__define_op(,   rx, queue),
+	__define_op(32,,    drinit),
+	__define_op(32,,    setupdd),
+};
+#endif /* CONFIG_SSB_DMA32 */
+
+#ifdef CONFIG_SSB_DMA64
+/* 64-bit tx channel ops */
+static const struct ssb_dma_ops ssb_dma64_tx_ops = {
+	__define_op(64,,    state),
+	__define_op(,   tx, queue),
+	__define_op(64,,    drinit),
+	__define_op(64,,    setupdd),
+};
+
+/* 64-bit rx channels ops */
+static const struct ssb_dma_ops ssb_dma64_rx_ops = {
+	__define_op(64,,    state),
+	__define_op(,   rx, queue),
+	__define_op(64,,    drinit),
+	__define_op(64,,    setupdd),
+};
+#endif /* CONFIG_SSB_DMA64 */
+
+/* free dma channel data */
+static void
+ssb_dma_free(struct ssb_dma *dma)
+{
+	ssb_dma_freepackets(dma);
+	kfree(dma->packets);
+
+	ssb_dma_free_coherent(dma, dma->drsize, dma->drv, dma->drp);
+	kfree(dma);
+}
+
+/* prepare dma channel data */
+static enum ssb_dma_result
+ssb_dma_init(struct ssb_dma *dma)
+{
+	enum ssb_dma_result res = ESSB_DMA_OK;
+	bool dma64, tx;
+
+	if ((dma->cfg->caps & SSB_DMA_64BIT) &&
+	    (dma->info.nr_slots < SSB_DMA_32RING_DD_MAXNR / 2) &&
+	    (dma->drboundary >= SSB_DMA_32RING_ALIGNBITS))
+		/* can relax boundary requirments for descriptor
+		 * tables never crossing ring boundary */
+		dma->drboundary--;
+
+	dma->packets = kcalloc(dma->info.nr_slots,
+			       sizeof(struct ssb_dma_packet), GFP_KERNEL);
+	if (!dma->packets) {
+		res = -ESSB_DMA_NOMEM;
+		goto done;
+	}
+
+	dma->drsize = dma->info.nr_slots * dma->cfg->ddsize;
+	dma->drv = ssb_dma_alloc_coherent(dma, dma->drsize, &dma->drp);
+	if (!dma->drv) {
+		res = -ESSB_DMA_DRINGALLOC;
+		goto err_free_pv;
+	}
+
+	SSB_DMA_BUG_ON(!dma->drp);
+	SSB_DMA_CHKMAPPING(dma, dma->cfg->drpb, dma->drp);
+
+	dma64 = dma->cfg->caps & SSB_DMA_64BIT;
+	tx = dma->info.dir == DMA_TO_DEVICE;
+	if (dma64) {
+#ifdef CONFIG_SSB_DMA64
+		dma->ops = tx ? &ssb_dma64_tx_ops : &ssb_dma64_rx_ops;
+#endif
+	} else {
+#ifdef CONFIG_SSB_DMA32
+		dma->ops = tx ? &ssb_dma32_tx_ops : &ssb_dma32_rx_ops;
+#endif
+	}
+	SSB_DMA_BUG_ON(!dma->ops);
+
+	memset(dma->drv, 0, dma->drsize);
+
+	/* setup last entry */
+	ssb_dma_setupdd(dma, dma->info.nr_slots - 1, 0, 0, 0);
+
+	dma->slot = dma->nr_posted = 0;
+
+	ssb_dma_cflags(dma, SSB_DMA_CTL_PE, 0);
+
+done:
+	return res;
+err_free_pv:
+	kfree(dma->packets);
+	dma->packets = NULL;
+	return res;
+}
+
+/** controller management routines **/
+
+/* initialization time controller wars, called after
+ * done with controller and channel probing */
+static enum ssb_dma_result
+ssb_dma_war(struct ssb_device *dev, const struct ssb_dma_info *info,
+	    struct ssb_dma_initdata *indata)
+{
+	struct ssb_bus *bus = dev->bus;
+#ifdef CONFIG_SSB_DRIVER_PCICORE
+	struct ssb_device *pcicore;
+#endif
+
+	switch (bus->bustype) {
+	case SSB_BUSTYPE_PCI:
+#ifdef CONFIG_SSB_DRIVER_PCICORE
+		pcicore = bus->pcicore.dev;
+		SSB_DMA_BUG_ON(!pcicore);
+		if (!(pcicore->id.coreid == SSB_DEV_PCIE &&
+		     (indata->caps & SSB_DMA_64BIT))) {
+			/* PCI (dma 32/dma 64) or PCIE with dma 32 */
+			u16 chip_id = bus->chip_id;
+			if ((chip_id == 4342) ||
+			    (chip_id == 0x4322) ||
+			    (chip_id == 43111) ||
+			    (chip_id == 43112) ||
+			    (chip_id == 43221) ||
+			    (chip_id == 43222) ||
+			    (chip_id == 43231)) {
+				indata->caps &= ~SSB_DMA_ADDRMASK;
+				indata->caps |= fls(SSB_PCI_DMA2) - 1;
+				indata->drpb.pa = SSB_PCI_DMA2;
+				indata->datapb.pa = indata->drpb.pa;
+#if defined(__mips__) && defined(CONFIG_CPU_BIG_ENDIAN)
+    				indata->datapb.lo += SSB_SDRAM_SWAPPED;
+#endif
+			}
+		}
+#endif /* CONFIG_SSB_DRIVER_PCICORE */
+		break;
+	default:
+		break;
+	}
+	return ESSB_DMA_OK;
+}
+
+/* probe channel capabilities */
+static enum ssb_dma_result
+ssb_dma_probe(struct ssb_device *dev, const struct ssb_dma_info *info,
+	      struct ssb_dma_initdata *indata)
+{
+	u32 caps = indata->caps;
+	u32 regs = info->regs;
+	u32 tmp;
+
+	SSB_DMA_BUG_ON(!regs);
+
+	if ((info->dir != DMA_TO_DEVICE) &&
+	    (info->dir != DMA_FROM_DEVICE))
+		return -ESSB_DMA_INVAL;
+
+	/* probe Address Extension support */
+	tmp = ssb_read32(dev, regs + SSB_DMA_CONTROL);
+	ssb_write32(dev, regs + SSB_DMA_CONTROL, tmp | SSB_DMA_CONTROL_AE_MASK);
+	tmp = ssb_read32(dev, regs + SSB_DMA_CONTROL);
+	if ((tmp & SSB_DMA_CONTROL_AE_MASK) == SSB_DMA_CONTROL_AE_MASK)
+		caps |= SSB_DMA_AE;
+	else
+		caps &= ~SSB_DMA_AE;
+
+	/* determine channel alignment requirments and
+	 * addressing capabilities */
+	if (indata->caps & SSB_DMA_64BIT) {
+		ssb_write32(dev, regs + SSB_DMA_64ADDRLO, 0x0FF0);
+		tmp = ssb_read32(dev, regs + SSB_DMA_64ADDRLO);
+		if (tmp) {
+			/* 16 byte alignment */
+			caps |= SSB_DMA_64BIT16BALIGN;
+			/* TODO: add support */
+			return -ESSB_DMA_UNSUPP;
+		}
+		else {
+			caps &= ~SSB_DMA_64BIT16BALIGN;
+		}
+	}
+
+	tmp = ssb_read32(dev, regs + SSB_DMA_CONTROL);
+	ssb_write32(dev, regs + SSB_DMA_CONTROL, tmp | SSB_DMA_CONTROL_PD);
+	/* parity check is supported if it can be disabled */
+	if (ssb_read32(dev, regs + SSB_DMA_CONTROL) & SSB_DMA_CONTROL_PD)
+		caps |= SSB_DMA_PARITY;
+	else
+		caps &= ~SSB_DMA_PARITY;
+
+	if (indata->nr_dma == 0)
+		indata->caps = caps;
+	else
+		/* This code is built around assumption that
+		 * all dma channels within single ssb device
+		 * share same capabilities. In order to support
+		 * devices (if ever such exist) which channels
+		 * have different caps we need to revork probing,
+		 * allocation and might be something else.
+		 * By now we just bug out. */
+		SSB_DMA_BUG_ON(caps != indata->caps);
+
+	return ESSB_DMA_OK;
+}
+
+/* identify basic dma controller capabilities */
+static enum ssb_dma_result
+ssb_dma_config_probe(struct ssb_device *dev, const struct ssb_dma_info *info,
+		     struct ssb_dma_initdata *indata)
+{
+	const struct ssb_dma_info *ci;
+	enum ssb_dma_result res;
+	bool core64 = ssb_core_state_flags(dev) & SSB_CORESTAT_DMA64;
+	bool ssb = dev->bus->bustype == SSB_BUSTYPE_SSB;
+	bool pci = dev->bus->bustype == SSB_BUSTYPE_PCI;
+	bool pcie = false;
+
+	/* Fail early if core support was not selected in kernel config */
+	if (core64) {
+#ifndef CONFIG_SSB_DMA64
+		return -ESSB_DMA_UNSUPP;
+#endif
+	} else {
+#ifndef CONFIG_SSB_DMA32
+		return -ESSB_DMA_UNSUPP;
+#endif
+	}
+
+#ifdef CONFIG_SSB_DRIVER_PCICORE
+	if (pci) {
+		struct ssb_device *pcicore = dev->bus->pcicore.dev;
+		SSB_DMA_BUG_ON(!pcicore);
+		pcie = pcicore->id.coreid == SSB_DEV_PCIE;
+	}
+#endif
+
+	/* check if device core is dma 64 capable */
+	if (core64) {
+		struct ssb_chipcommon *cc = &dev->bus->chipco;
+		/* 64-bit core registers layout */
+		indata->caps |= SSB_DMA_64BIT;
+		/* dma 64 is always 32 bits capable,
+		 * Address Extension is always true */
+		indata->caps &= ~SSB_DMA_ADDRMASK;
+		indata->caps |= 32 | SSB_DMA_AE;
+		/* check if backplane is dma 64 capable */
+		if (cc->capabilities & SSB_CHIPCO_CAP_64BIT)
+			if (pcie || ssb) {
+				/* If bus is plain SSB or SSB on PCIE
+				 * then we can access 64-bits */
+				indata->caps &= ~SSB_DMA_ADDRMASK;
+				indata->caps |= 64;
+			}
+		indata->max_slots = SSB_DMA_64RING_DD_MAXNR;
+		indata->ddsize = SSB_DMA_64RING_DD_SIZE;
+		indata->drboundary = SSB_DMA_64RING_ALIGNBITS;
+	} else {
+		/* Check for 32/30-bit addressing.
+		 * Address Extension support will be validated
+		 * later on during channel probe */
+		if (pcie)
+			indata->caps |= 32;
+		else if (pci || ssb)
+			indata->caps |= 30;
+		else
+			return -ESSB_DMA_UNSUPP;
+		indata->max_slots = SSB_DMA_32RING_DD_MAXNR;
+		indata->ddsize = SSB_DMA_32RING_DD_SIZE;
+		indata->drboundary = SSB_DMA_32RING_ALIGNBITS;
+	}
+
+	/* figure out the DMA physical address offset for descriptors
+	 * and data. PCI/PCIE map silicon backplane address to
+	 * zero based memory, need offset. for other buses use zero.
+	 * for SSB at big endian systems use sdram swapped region
+	 * for data buffers.
+	 *
+	 * instead of this and chunk in ssb_dma_controller_war()
+	 * we might need patch for ssb_dma_translation() in
+	 * drivers/ssb/main.c but it will impact b43/b44 so better be
+	 * tested somehow. */
+	if (pci) {
+		if (core64 && pcie) {
+			indata->drpb.lo = 0;
+#ifdef CONFIG_SSB_DMA64
+			indata->drpb.hi = SSB_PCIE_DMA_H32;
+#endif
+		} else {
+			/* pci dma 32/64 or pcie dma 32 */
+			indata->drpb.pa = SSB_PCI_DMA;
+		}
+		indata->datapb.pa = indata->drpb.pa;
+	};
+#if defined(__mips__) && defined(CONFIG_CPU_BIG_ENDIAN)
+	indata->datapb.lo += SSB_SDRAM_SWAPPED;
+#endif
+
+	/* check slots requested constraints, probe dma channels */
+	for (ci = info; ci->regs || ci->dir || ci->nr_slots;
+	     ci++, indata->nr_dma++) {
+		if (ci->nr_slots > indata->max_slots)
+			return -ESSB_DMA_INVAL;
+		if ((res = ssb_dma_probe(dev, ci, indata)))
+			return res;
+	}
+	SSB_DMA_BUG_ON(indata->nr_dma == 0);
+
+	indata->gfp = GFP_KERNEL;
+
+	/* checkout controller workarounds */
+	if ((res = ssb_dma_war(dev, info, indata)))
+		return res;
+
+	/* bug out if Address Extension is required
+	 * but its support not detected */
+	SSB_DMA_BUG_ON((indata->drpb.lo | indata->datapb.lo) &&
+		       !(indata->caps & SSB_DMA_AE));
+
+	return ESSB_DMA_OK;
+}
+
+/* release dma channels' data */
+void
+ssb_dma_close(struct ssb_dma_config *cfg)
+{
+	int i;
+	struct ssb_dma **dma;
+	struct ssb_dma_coherent_alloc_info *ca, *tmp;
+
+	for (i = 0, dma = &cfg->dma[0]; i < cfg->nr_dma; i++, dma++)
+		ssb_dma_free(*dma);
+
+	/* free coherent allocation fixups list */
+	list_for_each_entry_safe(ca, tmp, &cfg->ca_list, list) {
+		list_del(&ca->list);
+		kfree(ca);
+	};
+
+	kfree(cfg);
+}
+EXPORT_SYMBOL(ssb_dma_close);
+
+/* prepare dma channels' data */
+enum ssb_dma_result
+ssb_dma_open(struct ssb_device *sdev,
+	     const struct ssb_dma_info *info,
+	     struct ssb_dma_config **cfg)
+{
+	struct ssb_dma_initdata indata = {0,};
+	const struct ssb_dma_info *ci;
+	struct ssb_dma_config *new_cfg;
+	struct ssb_dma *dma, **dmatp;
+	enum ssb_dma_result res;
+	int err, size;
+	u64 dmamask;
+
+	if (!cfg)
+		return -ESSB_DMA_INVAL;
+
+	if ((res = ssb_dma_config_probe(sdev, info, &indata)))
+		return res;
+
+	/* TODO: fall down to dma32/dma30 if dma64/dma32 mask rejected */
+	dmamask = DMA_BIT_MASK(SSB_DMA_ADDRWIDTH(indata.caps));
+	if ((err = dma_set_mask(sdev->dma_dev, dmamask)))
+		return -ESSB_DMA_MASK;
+	err = dma_set_coherent_mask(sdev->dma_dev, dmamask);
+	SSB_DMA_BUG_ON(err);
+
+	/* allocate and initialize shared channel data */
+	size = sizeof(struct ssb_dma_config) +
+	       sizeof(struct ssb_dma *) * indata.nr_dma;
+	if (!(new_cfg = kzalloc(size, GFP_KERNEL)))
+		return -ESSB_DMA_NOMEM;
+	new_cfg->dev = sdev;
+	new_cfg->caps = indata.caps;
+	new_cfg->coherent_gfp = indata.gfp;
+	new_cfg->data_gfp = indata.gfp;
+	new_cfg->ddsize = indata.ddsize;
+	new_cfg->drpb = indata.drpb.pa;
+	new_cfg->datapb = indata.datapb.pa;
+	INIT_LIST_HEAD(&new_cfg->ca_list);
+
+	/* initialize channels */
+	for (ci = info, dmatp = &new_cfg->dma[0];
+	     new_cfg->nr_dma < indata.nr_dma;
+	     ci++, dmatp++, new_cfg->nr_dma++) {
+	    	size = ci->dir == DMA_TO_DEVICE ?
+	    		offsetof(struct ssb_dma, rx) :
+	    		sizeof(struct ssb_dma);
+	        dma = kzalloc(size, GFP_KERNEL);
+	        if (!dma) {
+	    		res = -ESSB_DMA_NOMEM;
+	    		goto err_free;
+	    	}
+	        dma->cfg = new_cfg;
+	        dma->idx = new_cfg->nr_dma;
+		dma->info = *ci;
+		dma->drboundary = indata.drboundary;
+		if ((res = ssb_dma_init(dma)))
+			goto err_free;
+	        *dmatp = dma;
+	}
+
+	*cfg = new_cfg;
+	return ESSB_DMA_OK;
+
+err_free:
+	ssb_dma_close(new_cfg);
+	return res;
+}
+EXPORT_SYMBOL(ssb_dma_open);
--- linux-3.0.4.orig/include/linux/ssb/ssb_dma.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-3.0.4/include/linux/ssb/ssb_dma.h	2011-09-25 00:05:45.000000000 +0300
@@ -0,0 +1,489 @@
+#ifndef LINUX_SSB_DMA_H_
+#define LINUX_SSB_DMA_H_
+
+#define ssb_dma_printk(level, fmt, args...) printk(level PFX fmt, ## args);
+#ifdef CONFIG_SSB_DMA_DEBUG
+# define ssb_dma_dprintk(level, fmt, args...) printk(level PFX "%s: " fmt, __FUNCTION__, ## args);
+# define SSB_DMA_TRACE(fmt, args...) ssb_dma_dprintk(KERN_INFO, fmt, ## args);
+# define SSB_DMA_INFO(fmt, args...) ssb_dma_dprintk(KERN_INFO, fmt, ## args);
+# define SSB_DMA_WARN(fmt, args...) ssb_dma_dprintk(KERN_WARNING, fmt, ## args);
+# define SSB_DMA_ERROR(fmt, args...) ssb_dma_dprintk(KERN_ERR, fmt, ## args);
+# define SSB_DMA_WARN_ON(x) WARN_ON(x)
+# define SSB_DMA_BUG_ON(x) do { if (x) { SSB_DMA_TRACE("BUG: %s:%d\n", __FILE__, __LINE__) }; BUG_ON(x); } while(0)
+# define SSB_DMA_NAME_MAXLEN	20
+#else
+# define SSB_DMA_INFO(fmt, args...) ssb_dma_printk(KERN_INFO, fmt, ## args);
+# define SSB_DMA_WARN(fmt, args...) ssb_dma_printk(KERN_WARNING, fmt, ## args);
+# define SSB_DMA_ERROR(fmt, args...) ssb_dma_printk(KERN_ERR, fmt, ## args);
+# define SSB_DMA_TRACE(fmt, args...) do {} while(0)
+# define SSB_DMA_WARN_ON(x) do {} while(0)
+# define SSB_DMA_BUG_ON(x) do {} while(0)
+#endif
+
+/*** Generic dma channel control flags ***/
+/* channel control register */
+#define SSB_DMA_CONTROL				0x00
+#define  SSB_DMA_CONTROL_OPEN			0x00000001 /* channel operating enable */
+#define  SSB_DMA_CONTROL_PD			0x00000800 /* parity check disable */
+#define  SSB_DMA_CONTROL_AE_MASK		0x00030000 /* address extension bits */
+#define  SSB_DMA_CONTROL_AE_SHIFT		16         /* address extension bits shift */
+/* tx-specific channel control flags */
+#define  SSB_DMA_CONTROL_TXSE			0x00000002 /* transmit suspend request */
+#define  SSB_DMA_CONTROL_TXLE			0x00000004 /* loopback enable */
+#define  SSB_DMA_CONTROL_TXFL			0x00000010 /* flush request */
+/* rx-specific channel control flags */
+#define  SSB_DMA_CONTROL_RXFO_MASK		0x000000FE /* frame offset mask */
+#define  SSB_DMA_CONTROL_RXFO_SHIFT		1          /* frame offset shift */
+#define  SSB_DMA_CONTROL_RXFM			0x00000100 /* direct fifo receive (pio) mode */
+#define  SSB_DMA_CONTROL_RXSH			0x00000200 /* separate rx header descriptor enable */
+#define  SSB_DMA_CONTROL_RXOC			0x00000400 /* overflow continue */
+
+/** 32-bit dma channel registers **/
+/* descriptor ring base address */
+#define SSB_DMA_32ADDR				0x04
+/* last descriptor pointer posted to chip */
+#define SSB_DMA_32PTR				0x08
+/* current/active descriptor pointer, channel state */
+#define SSB_DMA_32STATUS			0x0C
+#define  SSB_DMA_32STATUS_CDP_MASK		0x00000FFF /* current descriptor pointer mask */
+#define  SSB_DMA_32STATUS_STATE_MASK		0x0000F000 /* channel state mask */
+#define  SSB_DMA_32STATUS_STATE_SHIFT		12         /* channel state shift */
+#define  SSB_DMA_32STATUS_STATE_DISABLED	0x00000000 /* disabled */
+#define  SSB_DMA_32STATUS_STATE_ACTIVE		0x00001000 /* active */
+#define  SSB_DMA_32STATUS_STATE_IDLE		0x00002000 /* idle wait */
+#define  SSB_DMA_32STATUS_STATE_STOPPED		0x00003000 /* stopped */
+#define  SSB_DMA_32STATUS_STATE_SUSP		0x00004000 /* suspend pending */
+#define  SSB_DMA_32STATUS_ERR_MASK		0x000F0000 /* channel errors mask */
+#define  SSB_DMA_32STATUS_ERR_SHIFT		16         /* channel errors shift */
+#define  SSB_DMA_32STATUS_ERR_OK		0x00000000 /* no error */
+#define  SSB_DMA_32STATUS_ERR_DPE		0x00010000 /* descriptor protocol error */
+#define  SSB_DMA_32STATUS_ERR_DFE		0x00020000 /* data fifo error (tx underrun, rx overflow) */
+#define  SSB_DMA_32STATUS_ERR_DTE		0x00030000 /* data transfer error */
+#define  SSB_DMA_32STATUS_ERR_DRE		0x00040000 /* descriptor read error */
+#define  SSB_DMA_32STATUS_ERR_CORE		0x00050000 /* core error */
+#define  SSB_DMA_32STATUS_ADP_MASK		0xFFF00000 /* active descriptor pointer mask */
+#define  SSB_DMA_32STATUS_ADP_SHIFT		20         /* active descriptor pointer shift */
+/* current/active descriptor pointer, channel state/error macro defs */
+#define  SSB_DMA_32STATUS_CDP(s) ((s) & SSB_DMA_32STATUS_CDP_MASK)
+#define  SSB_DMA_32STATUS_ADP(s) (((s) & SSB_DMA_32STATUS_ADP_MASK) >> \
+					 SSB_DMA_32STATUS_ADP_SHIFT)
+#define  SSB_DMA_32STATUS_STATE(s) (((s) & SSB_DMA_32STATUS_STATE_MASK) >> \
+					   SSB_DMA_32STATUS_STATE_SHIFT)
+#define  SSB_DMA_32STATUS_ERR(s) (((s) & SSB_DMA_32STATUS_ERR_MASK) >> \
+					 SSB_DMA_32STATUS_ERR_SHIFT)
+
+/** 64-bit dma channel registers **/
+/* last descriptor pointer posted to chip */
+#define SSB_DMA_64PTR				0x04
+/* descriptor ring base address, low 32 bits */
+#define SSB_DMA_64ADDRLO			0x08
+/* descriptor ring base address, high 32 bits */
+#define SSB_DMA_64ADDRHI			0x0C
+/* current descriptor pointer, channel state */
+#define SSB_DMA_64STATUS0			0x10
+#define  SSB_DMA_64STATUS0_CDP_MASK		0x00001FFF /* current descriptor pointer */
+#define  SSB_DMA_64STATUS0_STATE_MASK		0xF0000000 /* channel state mask */
+#define  SSB_DMA_64STATUS0_STATE_SHIFT		28         /* channel state shift */
+#define  SSB_DMA_64STATUS0_STATE_DISABLED	0x00000000 /* disabled */
+#define  SSB_DMA_64STATUS0_STATE_ACTIVE		0x10000000 /* active */
+#define  SSB_DMA_64STATUS0_STATE_IDLE		0x20000000 /* idle wait */
+#define  SSB_DMA_64STATUS0_STATE_STOPPED	0x30000000 /* stopped */
+#define  SSB_DMA_64STATUS0_STATE_SUSP		0x40000000 /* suspend pending */
+/* active descriptor pointer, channel error */
+#define SSB_DMA_64STATUS1			0x14
+#define  SSB_DMA_64STATUS1_ADP_MASK		0x00001FFF /* active descriptor */
+#define  SSB_DMA_64STATUS1_ERR_MASK		0xF0000000 /* channel errors mask */
+#define  SSB_DMA_64STATUS1_ERR_SHIFT		28         /* channel errors shift */
+#define  SSB_DMA_64STATUS1_ERR_OK		0x00000000 /* no error */
+#define  SSB_DMA_64STATUS1_ERR_DPE		0x10000000 /* descriptor protocol error */
+#define  SSB_DMA_64STATUS1_ERR_DFE		0x20000000 /* data fifo error (tx underrun, rx overflow) */
+#define  SSB_DMA_64STATUS1_ERR_DTE		0x30000000 /* data transfer error */
+#define  SSB_DMA_64STATUS1_ERR_DRE		0x40000000 /* descriptor read error */
+#define  SSB_DMA_64STATUS1_ERR_CORE		0x50000000 /* core error */
+/* current/active descriptor pointer, channel state/error macro defs */
+#define  SSB_DMA_64STATUS0_CDP(s0) ((s0) & SSB_DMA_64STATUS0_CDP_MASK)
+#define  SSB_DMA_64STATUS1_ADP(s1) ((s1) & SSB_DMA_64STATUS1_ADP_MASK)
+#define  SSB_DMA_64STATUS0_STATE(s0) (((s0) & SSB_DMA_64STATUS0_STATE_MASK) >> \
+					      SSB_DMA_64STATUS0_STATE_SHIFT)
+#define  SSB_DMA_64STATUS1_ERR(s1) (((s1) & SSB_DMA_64STATUS1_ERR_MASK) >> \
+					    SSB_DMA_64STATUS1_ERR_SHIFT)
+
+/* 32-bit ring alignments and sizes */
+#define SSB_DMA_32RING_ALIGNBITS	12
+#define SSB_DMA_32RING_ALIGN		(1 << SSB_DMA_32RING_ALIGNBITS) /* Must be 4Kb aligned */
+#define SSB_DMA_32RING_SIZE		(1 << SSB_DMA_32RING_ALIGNBITS) /* Must fit within a single 4Kb page */
+#define SSB_DMA_32RING_DD_SIZE		8				/* 32-bit descriptor size */
+#define SSB_DMA_32RING_DD_MAXNR		SSB_DMA_32RING_SIZE / \
+					SSB_DMA_32RING_DD_SIZE
+
+/* 64-bit ring alignments and sizes */
+#define SSB_DMA_64RING_ALIGNBITS	13
+#define SSB_DMA_64RING_ALIGN		(1 << SSB_DMA_64RING_ALIGNBITS) /* Must be 8Kb aligned */
+#define SSB_DMA_64RING_SIZE		(1 << SSB_DMA_64RING_ALIGNBITS) /* Must fit within a single 8Kb page */
+#define SSB_DMA_64RING_DD_SIZE		16				/* 64-bit descriptor size */
+#define SSB_DMA_64RING_DD_MAXNR		SSB_DMA_64RING_SIZE / \
+					SSB_DMA_64RING_DD_SIZE
+
+/* generic 32/64 bit dma descriptor flags */
+#define SSB_DMA_DDCTRL_EOT		0x10000000	/* end of descriptor table */
+#define SSB_DMA_DDCTRL_IOC		0x20000000	/* interrupt on completion */
+#define SSB_DMA_DDCTRL_EOF		0x40000000	/* end of frame */
+#define SSB_DMA_DDCTRL_SOF		0x80000000	/* start of frame */
+#define SSB_DMA_DDCTRL_FLAGS_MASK	(SSB_DMA_DDCTRL_EOT | \
+					 SSB_DMA_DDCTRL_IOC | \
+					 SSB_DMA_DDCTRL_EOF | \
+					 SSB_DMA_DDCTRL_SOF)
+
+/* 32-bit dma descriptor */
+struct ssb_dma_dd32 {
+	__le32 control; /* SSB_DMA_32DDCTRL_xx control flags and buffer length */
+	__le32 address; /* translated data buffer physical address */
+} __packed;
+
+/* 32-bit dma descriptor control flags */
+#define SSB_DMA_32DDCTRL_BC_MASK	0x00001FFF	/* buffer byte count, real data length must be <= 4KB */
+#define SSB_DMA_32DDCTRL_AE_MASK	SSB_DMA_CONTROL_AE_MASK
+#define SSB_DMA_32DDCTRL_AE_SHIFT	SSB_DMA_CONTROL_AE_SHIFT
+#define SSB_DMA_32DDCTRL_PARITY		0x000C0000	/* FIXME: parity bit */
+#define SSB_DMA_32DDCTRL_EOT		SSB_DMA_DDCTRL_EOT
+#define SSB_DMA_32DDCTRL_IOC		SSB_DMA_DDCTRL_IOC
+#define SSB_DMA_32DDCTRL_EOF		SSB_DMA_DDCTRL_EOF
+#define SSB_DMA_32DDCTRL_SOF		SSB_DMA_DDCTRL_SOF
+#define SSB_DMA_32DDCTRL_CORE_MASK	0x0FF00000	/* core-specific control flags mask */
+
+/* 64 bit dma descriptor */
+struct ssb_dma_dd64 {
+	__le32 control0; /* SSB_DMA_64DDCTRL0_xx control flags */
+	__le32 control1; /* address extension and parity bits, buffer length*/
+	__le32 address_lo; /* translated data buffer physical address, low 32 bits */
+	__le32 address_hi; /* translated data buffer physical address, high 32 bits*/
+} __packed;
+
+/* 64-bit dma descriptor control0 flags */
+#define SSB_DMA_64DDCTRL0_EOT		SSB_DMA_DDCTRL_EOT
+#define SSB_DMA_64DDCTRL0_IOC		SSB_DMA_DDCTRL_IOC
+#define SSB_DMA_64DDCTRL0_EOF		SSB_DMA_DDCTRL_EOF
+#define SSB_DMA_64DDCTRL0_SOF		SSB_DMA_DDCTRL_SOF
+#define SSB_DMA_64DDCTRL0_CORE_MASK	0x0FF00000	/* core-specific control flags mask */
+
+/* 64-bit dma descriptor control1 flags */
+#define SSB_DMA_64DDCTRL1_BC_MASK	0x00007FFF	/* buffer byte count. real data len must be <= 16KB */
+#define SSB_DMA_64DDCTRL1_AE_MASK	SSB_DMA_CONTROL_AE_MASK
+#define SSB_DMA_64DDCTRL1_AE_SHIFT	SSB_DMA_CONTROL_AE_SHIFT
+#define SSB_DMA_64DDCTRL1_PARITY	0x00040000	/* parity bit */
+
+/* generic rx channel frame header flags */
+/* TODO:  */
+#define SSB_DMA_RXFRAME_OFLO		0x0080	/* rx overflow */
+#define SSB_DMA_RXFRAME_DDNUM_MASK	0x0F00  /* number of descriptors used mask */
+#define SSB_DMA_RXFRAME_DDNUM_SHIFT	8	/* number of descriptors used shift */
+#define SSB_DMA_RXFRAME_DT		0xF000	/* core-dependent data type */
+
+#ifdef CONFIG_SSB_DMA_DEBUG
+# define DEFINE_SSB_DMA_NAME(n) .name = (n),
+# define SSB_DMA_TABLE_END { { 0, } },
+#else
+# define DEFINE_SSB_DMA_NAME(n)
+# define SSB_DMA_TABLE_END { 0, },
+#endif
+
+/* dma channel definition macro defs */
+#define SSB_DMA(n,r,s,d) \
+	{ DEFINE_SSB_DMA_NAME(n) .regs = (r), .nr_slots = (s), .dir = (d), }
+
+/* channel information */
+struct ssb_dma_info {
+#ifdef CONFIG_SSB_DMA_DEBUG
+	/* channel name (for debugging purposes) */
+	char name[SSB_DMA_NAME_MAXLEN];
+#endif
+	/* io registers offset relative to owning device registers base */
+	u16 regs;
+	/* number of xmit slots requested */
+	u16 nr_slots;
+	/* direction (only DMA_TO_DEVICE and DMA_FROM_DEVICE are supported). */
+	enum dma_data_direction dir;
+};
+
+/* ssb dma sw capabilities */
+#define SSB_DMA_ADDRMASK		0x007F
+#define SSB_DMA_ADDRWIDTH(caps) ((caps) & SSB_DMA_ADDRMASK)
+#define SSB_DMA_64BIT			0x0100 /* 64-bit core */
+#define SSB_DMA_AE			0x0200 /* Address Extension bits support */
+#define SSB_DMA_PARITY			0x0400 /* parity check support */
+#define SSB_DMA_64BIT16BALIGN		0x0800 /* 64-bit core with 16-byte descriptor
+						* pointer alignment */
+
+/* ssb dma channel sw control flags */
+#define SSB_DMA_CTL_PE			0x01	/* parity check enable */
+#define SSB_DMA_CTL_RXOC		0x02	/* receive overflow continue (rx only) */
+#ifdef CONFIG_SSB_DMA_DEBUG
+#define SSB_DMA_CTL_RXMASK		SSB_DMA_CTL_RXOC
+#define SSB_DMA_CTL_TXMASK		0
+#endif
+
+/* dma channel hw state (translation from core-dependent
+ * SSB_DMA_32STATUS_STATE_xx/SSB_DMA_64STATUS0_STATE_xx) */
+enum ssb_dma_state {
+	SSB_DMA_STATE_DISABLED,	/* disabled */
+	SSB_DMA_STATE_ACTIVE,	/* active */
+	SSB_DMA_STATE_IDLE,	/* idle wait */
+	SSB_DMA_STATE_STOPPED,	/* operating stopped */
+	SSB_DMA_STATE_SUSP,	/* operating suspend pending */
+};
+
+/* dma channel hw error (translation from core-dependent
+ * SSB_DMA_32STATUS_ERR_xx/SSB_DMA_64STATUS1_ERR_xx) */
+enum ssb_dma_error {
+	SSB_DMA_ERR_OK,		/* no error */
+	SSB_DMA_ERR_DPE,	/* descriptor protocol error */
+	SSB_DMA_ERR_DFE,	/* data fifo error (tx underrun or rx overflow) */
+	SSB_DMA_ERR_DTE,	/* data transfer (tx/rx) error */
+	SSB_DMA_ERR_DRE,	/* descriptor read error */
+	SSB_DMA_ERR_CORE,	/* core error */
+};
+
+/* dma channel sw result codes */
+enum ssb_dma_result {
+	ESSB_DMA_OK,		/* no error */
+	ESSB_DMA_UNSPEC,	/* non specific error */
+	ESSB_DMA_INVAL,		/* invalid parameter */
+	ESSB_DMA_UNSUPP,	/* unsupported dma controller configuration */
+	ESSB_DMA_NOMEM,		/* out of memory */
+	ESSB_DMA_COREFAIL,	/* dma core failure */
+	ESSB_DMA_TIMEDOUT,	/* dma operation timeout */
+	ESSB_DMA_PROBE,		/* probe failure */
+	ESSB_DMA_DRINGALLOC,	/* descriptor ring coherent alloc */
+	ESSB_DMA_DRINGEMPTY,	/* descriptor ring empty */
+	ESSB_DMA_MASK,		/* dma/coherent dma mask */
+	ESSB_DMA_DATAMAP,	/* data mapping */
+	ESSB_DMA_TRANSLATION,	/* address translation for dma32 or
+				 * dma64 with 32-bit address extension */
+	ESSB_DMA_NOFREESLOTS,	/* out of free slots */
+	ESSB_DMA_RXNOPENDING,	/* no packets received yet */
+};
+
+/* ssb dma physical address descriptor */
+#ifdef CONFIG_SSB_DMA64
+typedef u64 ssb_dma_addr_t;
+#else
+typedef u32 ssb_dma_addr_t;
+#endif
+
+/* shared dma channel configuration data */
+struct ssb_dma_config {
+	/* owning device */
+	struct ssb_device *dev;
+
+	u8 ddsize; /* single dma descriptor size */
+
+	/* number of underlying channels (could be less than
+	 * actual physical channels), as only requested channel
+	 * set is being initialized */
+	u8 nr_dma;
+
+	/* channel capabilities. see SSB_DMAxxxx defines */
+	u32 caps;
+
+	gfp_t coherent_gfp; /* dma coherent memory allocation kernel flags */
+	gfp_t data_gfp; /* dma buffer allocations kernel flags */
+
+	/* descriptor ring translation base for PCI controllers
+	 * which require physical addresses offseting */
+	ssb_dma_addr_t drpb;
+	/* data translation base for PCI controllers
+	 * which require physical addresses offseting */
+	ssb_dma_addr_t datapb;
+
+	/* list of coherent allocated blocks, which were
+	 * workarounded to meet dma controller alignment
+	 * requirments (struct ssb_dma_coherent_alloc_info) */
+	struct list_head ca_list;
+
+	/* here start channels' data pointers array.
+	 * keep it last field in this struct */
+	struct ssb_dma *dma[0];
+};
+
+/* single dma channel data */
+struct ssb_dma {
+	/* shared channel config */
+	struct ssb_dma_config *cfg;
+	/* channel index in owning config table */
+	u8 idx;
+
+	/* channel info struct copy channel was initialized with */
+	struct ssb_dma_info info;
+
+	/* driver-specific data pointer */
+	void *drvdata;
+	/* device driver ops */
+	struct ssb_dma_driver_ops *driver_ops;
+
+	/* number of descriptors posted */
+	u16 nr_posted;
+
+	/** private area **/
+	/* channel control flags, see SSB_DMA_CHANNEL_xxxx defines.
+	 * dont alter flags directly, use ssb_dma_channel_cflags
+	 * instead to ensure proper channel state handling and
+	 * debugging sanity checks. */
+	u32 cflags;
+
+	/* descriptor ring boundary, bits */
+	u8 drboundary;
+
+	/* descriptor ring allocation size */
+	u16 drsize;
+	/* descriptor ring physical address */
+	dma_addr_t drp;
+	/* descriptor ring virtual address */
+	union {
+		void *drv;
+		struct ssb_dma_dd32 *drv32;
+		struct ssb_dma_dd64 *drv64;
+	};
+
+	/* pointer to packets' vector */
+	struct ssb_dma_packet *packets;
+
+	/* current descriptor index */
+	s16 slot;
+	/* channel ops */
+	const struct ssb_dma_ops *ops;
+
+	/** rx-only channel data **/
+	struct {
+		/* rx frame size */
+		u16 framesize;
+		/* core-specific frame header size */
+		u16 hdrsize;
+	} rx;
+};
+
+/* device driver ops */
+struct ssb_dma_driver_ops {
+	void *(*buf_alloc)(struct ssb_dma *dma, u16 length, void **data);
+	void (*buf_free)(struct ssb_dma *dma, void *buffer);
+};
+
+/** exported routines **/
+/* ssb_dma_state() - query channel state
+ */
+extern enum ssb_dma_result
+ssb_dma_state(struct ssb_dma *dma,
+	      enum ssb_dma_state *status,
+	      enum ssb_dma_error *error);
+
+/* ssb_dma_enable() - try to enable channel
+ */
+extern enum ssb_dma_result
+ssb_dma_enable(struct ssb_dma *dma);
+
+/* ssb_dma_reset() - reset channel to defaults
+ */
+extern enum ssb_dma_result
+ssb_dma_reset(struct ssb_dma *dma);
+
+/* ssb_dma_disable() - try to suspend channel
+ */
+extern enum ssb_dma_result
+ssb_dma_disable(struct ssb_dma *dma);
+
+/* ssb_dma_queue() - process channel pending queue
+ */
+extern enum ssb_dma_result
+ssb_dma_queue(struct ssb_dma *dma);
+
+/* ssb_dma_cflags() - alter channel control flags
+ */
+extern void
+ssb_dma_cflags(struct ssb_dma *dma, u32 mask, u32 flags);
+
+/* ssb_dma_enabled() - check if channel is enabled
+ */
+extern bool
+ssb_dma_enabled(struct ssb_dma *dma);
+
+/* ssb_dma_suspended() - check if channel is suspended (tx only)
+ */
+extern bool
+ssb_dma_suspended(struct ssb_dma *dma);
+
+/* ssb_dma_suspend() - suspend channel (tx only)
+ */
+extern void
+ssb_dma_suspend(struct ssb_dma *dma);
+
+/* ssb_dma_resume() - resume previously suspended channel (tx only)
+ */
+extern void
+ssb_dma_resume(struct ssb_dma *dma);
+
+/* ssb_dma_queue() - process pending queue
+ */
+extern enum ssb_dma_result
+ssb_dma_queue(struct ssb_dma *dma);
+
+/* ssb_dma_txinit() - prepare tx channel
+ */
+extern enum ssb_dma_result
+ssb_dma_txinit(struct ssb_dma *dma, struct ssb_dma_driver_ops *ops);
+
+/* ssb_dma_txsingle() - post packet for transmitting
+ */
+extern enum ssb_dma_result
+ssb_dma_txsingle(struct ssb_dma *dma, void *buffer, void *data,
+		 u16 length, bool commit);
+
+/* ssb_dma_rxinit() - prepare rx channel
+ */
+extern enum ssb_dma_result
+ssb_dma_rxinit(struct ssb_dma *dma, struct ssb_dma_driver_ops *ops,
+	       u16 bufsize, u16 hdrsize);
+
+/* ssb_dma_rxsingle() - get next packet received
+ */
+extern enum ssb_dma_result
+ssb_dma_rxsingle(struct ssb_dma *dma, void **buffer);
+
+/* ssb_dma_open() - allocate and initialize ssb dma channel's data
+ *
+ * @dev:    ssb device owning dma channels
+ * @info:   channels' being requested descriptions
+ * @config: pointer to value receiving dma channels' configuration
+ *          data record pointer on success
+ */
+extern enum ssb_dma_result
+ssb_dma_open(struct ssb_device *sdev, const struct ssb_dma_info *info,
+	     struct ssb_dma_config **cfg);
+
+/* ssb_dma_close() - free ssb dma channels data
+ */
+extern void
+ssb_dma_close(struct ssb_dma_config *cfg);
+
+/** misc. inline helpers **/
+/* ssb_dma_set_drvdata() - set driver specific channel data
+ */
+static inline void
+ssb_dma_set_drvdata(struct ssb_dma *dma, void *data)
+{
+	dma->drvdata = data;
+}
+
+/* ssb_dma_get_drvdata() - get driver specific channel data
+ */
+static inline void *
+ssb_dma_get_drvdata(struct ssb_dma *dma, void *data)
+{
+	return dma->drvdata;
+}
+
+#ifdef CONFIG_SSB_DMA_DEBUG
+/** misc. debugging stuff **/
+extern void
+ssb_dma_dumpinfo(struct ssb_dma *dma, bool verbose);
+#endif
+
+#endif /* !LINUX_SSB_DMA_H_ */
--- linux-3.0.4.orig/include/linux/ssb/ssb_regs.h	2011-09-25 00:05:34.000000000 +0300
+++ linux-3.0.4/include/linux/ssb/ssb_regs.h	2011-09-25 00:05:45.000000000 +0300
@@ -20,6 +20,7 @@
 #define	SSB_FLASH1_SZ		0x00400000U	/* Size of Flash Region 1 */
 
 #define SSB_PCI_DMA		0x40000000U	/* Client Mode sb2pcitranslation2 (1 GB) */
+#define SSB_PCI_DMA2		0x80000000U	/* Client Mode sb2pcitranslation2 (1 GB) */
 #define SSB_PCI_DMA_SZ		0x40000000U	/* Client Mode sb2pcitranslation2 size in bytes */
 #define SSB_PCIE_DMA_L32	0x00000000U	/* PCIE Client Mode sb2pcitranslation2 (2 ZettaBytes), low 32 bits */
 #define SSB_PCIE_DMA_H32	0x80000000U	/* PCIE Client Mode sb2pcitranslation2 (2 ZettaBytes), high 32 bits */
